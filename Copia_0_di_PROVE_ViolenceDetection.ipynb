{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copia_0_di_PROVE_ViolenceDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esPqye6MMEs4"
      },
      "source": [
        "# **VIOLENCE DETECTION V1.0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTT2x9YGnY9T"
      },
      "source": [
        "----------------------------------------------------------------\n",
        "# **CONNECTION** (google drive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYIPv6fOmomh",
        "outputId": "32dc35d2-62da-4a75-8975-d2919f2f8b0c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive\n",
            "'Colab Notebooks'  'Diagram CivicSense.vpd'   filmando-eng.rar\t lib.rar\n",
            " CUSTOM.gsheet\t    filesFilmando2\t      GPUs.gsheet\n",
            " data\t\t    filesFilmando2.rar\t     'laurea erika'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-idtdsGKbYO"
      },
      "source": [
        "# **DATA PREPROCESSING**\n",
        "As a **preparation** for the **graph input** few steps were taken in the dataset preparation\n",
        "- initially the **videos** were **sampled** to a frame by **frame sequence** as we were limited with computational power. \n",
        "- The videos were sampled into a **fix number** of **frames** before given as an input to the model. \n",
        "\n",
        "- For all dataset combination of **augmentation methods** were used and for some of the datasets, **dark edges** were **removed** from the frame as we present in Figure 3.\n",
        "- As the original article stated, the **input** to the model is a **subtraction** of **adjacent frames**, this was done in order to include a **spatial movements** in the input videos instead of the raw pixels from each frame. \n",
        "- In Figure 2 we present and example of difference computation of adjacent frames where an hockey player pushes another player.\n",
        "\n",
        "-To enrich and **enlarge** the **dataset** we apply **data augmentation** with the following\n",
        "**transformations** on the frames:\n",
        "  - **Image cropping**: a **slicing** of the **image**, done each time with a **different** anchor **corner** was chosen (Figure 4) .\n",
        "  - **Image transpose**: as a complement steps to the cropping process, a transpose was done, this **step** was done during the **fit generator process** (Figure 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RefAlxwLdlrp"
      },
      "source": [
        "**SAVE_FIGURES_FROM_VIDEO** (required for CREATE DATASET)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvFG8rxIoS_1"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "def save_figures_from_video(dataset_video_path, video_filename, suffix,figures_path,skip_frames = 25,apply_norm = True, apply_diff = True,fix_len = None):\n",
        "    seq_len = 0\n",
        "\n",
        "    video_figures_path = os.path.join(figures_path ,video_filename)\n",
        "    if not os.path.exists(video_figures_path):\n",
        "        os.makedirs(video_figures_path)\n",
        "\n",
        "    video_file = os.path.join(dataset_video_path, video_filename + suffix)\n",
        "    label = 0\n",
        "    #print('Extracting frames from video: ', video_file)\n",
        "    print('.', end='')\n",
        "\n",
        "    videoCapture = cv2.VideoCapture(video_file)\n",
        "    if fix_len is not None:\n",
        "        vid_len = int(videoCapture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        skip_frames = int(float(vid_len)/float(fix_len))\n",
        "    videoCapture.set(cv2.CAP_PROP_POS_MSEC, (seq_len * skip_frames))\n",
        "    success, figure_ = videoCapture.read()\n",
        "    success = True\n",
        "    files = []\n",
        "    while success:\n",
        "        success, figure = videoCapture.read()\n",
        "\n",
        "        if seq_len % skip_frames == 0:\n",
        "            if success:\n",
        "                figure_curr = figure\n",
        "                image_file = os.path.join(video_figures_path , \"frame_%d.jpg\" % seq_len)\n",
        "                files.append(image_file)\n",
        "                cv2.imwrite(image_file, figure_curr)\n",
        "        seq_len += 1\n",
        "    video_images = dict(images_path = video_figures_path, name = video_filename,\n",
        "                        images_files = files, sequence_length = seq_len, label = label)\n",
        "\n",
        "    return video_images\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuABj-PO5FNZ",
        "outputId": "d56c7092-c0fd-42b2-debe-e05f46346629"
      },
      "source": [
        "import pickle\n",
        "\n",
        "datasets_videos = dict(\n",
        "    hocky = dict(hocky=\"data/raw_videos/HockeyFights\"),\n",
        "    violentflow = dict(violentflow=\"data/raw_videos/violentflow\"),\n",
        "    movies = dict(movies=\"data/raw_videos/movies\")\n",
        ")\n",
        "\n",
        "#for dataset_name, dataset_videos in datasets_videos.items():\n",
        "dataset_name = \"movies\"                          #         \"movies\"  \"violentflow\"   \"hocky\"\n",
        "dataset_videos = datasets_videos.get(\"movies\")   #         \"movies\"  \"violentflow\"   \"hocky\"\n",
        "print(dataset_name, dataset_videos)\n",
        "\n",
        "force=False           #True to force rebuilding dataset\n",
        "videos_seq_length = []\n",
        "datasets_images = {}\n",
        "videos_frames_paths = []\n",
        "videos_labels = []\n",
        "datasets_video_path = dataset_videos\n",
        "figure_output_path = \"data/raw_frames\"\n",
        "\n",
        "#Extract images for each video for the dataset\n",
        "for dataset_name, dataset_video_path in datasets_video_path.items():\n",
        "    dataset_figures_path = os.path.join(figure_output_path,dataset_name)\n",
        "    if not os.path.exists(dataset_figures_path):\n",
        "        os.makedirs(dataset_figures_path)\n",
        "    dataset_images = []\n",
        "    \n",
        "    for filename in os.listdir(dataset_video_path):\n",
        "        if filename.endswith(\".avi\") or filename.endswith(\".mpg\"):\n",
        "            video_images_file = os.path.join(dataset_figures_path,filename[:-4], 'video_summary.pkl')\n",
        "            if os.path.isfile(video_images_file) and not force:\n",
        "                with open(video_images_file, 'rb') as f:\n",
        "                    video_images = pickle.load(f)               #load dump of frames already decomposed\n",
        "            else:\n",
        "                video_images = save_figures_from_video(dataset_video_path, filename[:-4],filename[-4:], dataset_figures_path, fix_len = 20)\n",
        "                if dataset_name == \"hocky\":\n",
        "                    if filename.startswith(\"fi\"):\n",
        "                        video_images['label'] = 1\n",
        "                elif dataset_name == \"violentflow\":\n",
        "                    if \"violence\" in filename:\n",
        "                        video_images['label'] = 1\n",
        "                elif dataset_name == \"movies\":\n",
        "                    if \"fi\" in filename:\n",
        "                        video_images['label'] = 1\n",
        "                with open(video_images_file, 'wb') as f:\n",
        "                    pickle.dump(video_images, f, pickle.HIGHEST_PROTOCOL)\n",
        "            dataset_images.append(video_images)\n",
        "            videos_seq_length.append(video_images['sequence_length'])\n",
        "            videos_frames_paths.append(video_images['images_path'])\n",
        "            videos_labels.append(video_images['label'])\n",
        "    datasets_images[dataset_name] = dataset_images\n",
        "avg_length = int(float(sum(videos_seq_length)) / max(len(videos_seq_length), 1))\n",
        "\n",
        "#print(videos_seq_length)\n",
        "print(videos_frames_paths)\n",
        "print(videos_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "movies {'movies': 'data/raw_videos/movies'}\n",
            "['data/raw_frames/movies/10', 'data/raw_frames/movies/100', 'data/raw_frames/movies/11', 'data/raw_frames/movies/1', 'data/raw_frames/movies/13', 'data/raw_frames/movies/14', 'data/raw_frames/movies/16', 'data/raw_frames/movies/15', 'data/raw_frames/movies/17', 'data/raw_frames/movies/12 (1)', 'data/raw_frames/movies/2', 'data/raw_frames/movies/18', 'data/raw_frames/movies/19', 'data/raw_frames/movies/21', 'data/raw_frames/movies/22', 'data/raw_frames/movies/23', 'data/raw_frames/movies/24', 'data/raw_frames/movies/26 (1)', 'data/raw_frames/movies/25', 'data/raw_frames/movies/20 (1)', 'data/raw_frames/movies/27', 'data/raw_frames/movies/28', 'data/raw_frames/movies/29', 'data/raw_frames/movies/31', 'data/raw_frames/movies/30', 'data/raw_frames/movies/33 (1)', 'data/raw_frames/movies/3', 'data/raw_frames/movies/32', 'data/raw_frames/movies/33', 'data/raw_frames/movies/34', 'data/raw_frames/movies/36', 'data/raw_frames/movies/35', 'data/raw_frames/movies/37', 'data/raw_frames/movies/38', 'data/raw_frames/movies/40', 'data/raw_frames/movies/39', 'data/raw_frames/movies/42 (1)', 'data/raw_frames/movies/43 (1)', 'data/raw_frames/movies/41 (1)', 'data/raw_frames/movies/44', 'data/raw_frames/movies/45', 'data/raw_frames/movies/46', 'data/raw_frames/movies/48', 'data/raw_frames/movies/47', 'data/raw_frames/movies/49', 'data/raw_frames/movies/50', 'data/raw_frames/movies/5', 'data/raw_frames/movies/51', 'data/raw_frames/movies/53', 'data/raw_frames/movies/57', 'data/raw_frames/movies/58', 'data/raw_frames/movies/54', 'data/raw_frames/movies/59', 'data/raw_frames/movies/6', 'data/raw_frames/movies/55', 'data/raw_frames/movies/61', 'data/raw_frames/movies/56', 'data/raw_frames/movies/60', 'data/raw_frames/movies/62', 'data/raw_frames/movies/63', 'data/raw_frames/movies/64', 'data/raw_frames/movies/65', 'data/raw_frames/movies/66', 'data/raw_frames/movies/68', 'data/raw_frames/movies/69', 'data/raw_frames/movies/7', 'data/raw_frames/movies/70', 'data/raw_frames/movies/67 (1)', 'data/raw_frames/movies/71avi', 'data/raw_frames/movies/72', 'data/raw_frames/movies/73', 'data/raw_frames/movies/75', 'data/raw_frames/movies/8', 'data/raw_frames/movies/74', 'data/raw_frames/movies/76', 'data/raw_frames/movies/77', 'data/raw_frames/movies/79', 'data/raw_frames/movies/78', 'data/raw_frames/movies/82', 'data/raw_frames/movies/81', 'data/raw_frames/movies/80', 'data/raw_frames/movies/83', 'data/raw_frames/movies/84', 'data/raw_frames/movies/87', 'data/raw_frames/movies/85', 'data/raw_frames/movies/86', 'data/raw_frames/movies/88', 'data/raw_frames/movies/89', 'data/raw_frames/movies/9', 'data/raw_frames/movies/90', 'data/raw_frames/movies/98', 'data/raw_frames/movies/97', 'data/raw_frames/movies/91', 'data/raw_frames/movies/92', 'data/raw_frames/movies/93', 'data/raw_frames/movies/94', 'data/raw_frames/movies/95', 'data/raw_frames/movies/96', 'data/raw_frames/movies/99', 'data/raw_frames/movies/4 (1)', 'data/raw_frames/movies/52', 'data/raw_frames/movies/newfi1', 'data/raw_frames/movies/newfi10', 'data/raw_frames/movies/newfi100', 'data/raw_frames/movies/newfi11', 'data/raw_frames/movies/newfi12', 'data/raw_frames/movies/newfi13', 'data/raw_frames/movies/newfi14', 'data/raw_frames/movies/newfi16', 'data/raw_frames/movies/newfi15', 'data/raw_frames/movies/newfi17', 'data/raw_frames/movies/newfi18', 'data/raw_frames/movies/newfi19', 'data/raw_frames/movies/newfi2', 'data/raw_frames/movies/newfi20', 'data/raw_frames/movies/newfi21', 'data/raw_frames/movies/newfi22', 'data/raw_frames/movies/newfi23', 'data/raw_frames/movies/newfi24', 'data/raw_frames/movies/newfi25', 'data/raw_frames/movies/newfi26', 'data/raw_frames/movies/newfi27', 'data/raw_frames/movies/newfi28', 'data/raw_frames/movies/newfi29', 'data/raw_frames/movies/newfi3', 'data/raw_frames/movies/newfi30', 'data/raw_frames/movies/newfi31', 'data/raw_frames/movies/newfi32', 'data/raw_frames/movies/newfi33', 'data/raw_frames/movies/newfi34', 'data/raw_frames/movies/newfi35', 'data/raw_frames/movies/newfi36', 'data/raw_frames/movies/newfi37', 'data/raw_frames/movies/newfi38', 'data/raw_frames/movies/newfi39', 'data/raw_frames/movies/newfi4', 'data/raw_frames/movies/newfi41', 'data/raw_frames/movies/newfi40', 'data/raw_frames/movies/newfi42', 'data/raw_frames/movies/newfi43', 'data/raw_frames/movies/newfi44', 'data/raw_frames/movies/newfi45', 'data/raw_frames/movies/newfi46', 'data/raw_frames/movies/newfi47', 'data/raw_frames/movies/newfi48', 'data/raw_frames/movies/newfi49', 'data/raw_frames/movies/newfi5', 'data/raw_frames/movies/newfi50', 'data/raw_frames/movies/newfi51', 'data/raw_frames/movies/newfi52', 'data/raw_frames/movies/newfi53', 'data/raw_frames/movies/newfi54', 'data/raw_frames/movies/newfi55', 'data/raw_frames/movies/newfi56', 'data/raw_frames/movies/newfi57', 'data/raw_frames/movies/newfi58', 'data/raw_frames/movies/newfi59', 'data/raw_frames/movies/newfi6', 'data/raw_frames/movies/newfi60', 'data/raw_frames/movies/newfi61', 'data/raw_frames/movies/newfi62', 'data/raw_frames/movies/newfi63', 'data/raw_frames/movies/newfi64', 'data/raw_frames/movies/newfi65', 'data/raw_frames/movies/newfi66', 'data/raw_frames/movies/newfi68', 'data/raw_frames/movies/newfi67', 'data/raw_frames/movies/newfi69', 'data/raw_frames/movies/newfi70', 'data/raw_frames/movies/newfi7', 'data/raw_frames/movies/newfi71', 'data/raw_frames/movies/newfi72', 'data/raw_frames/movies/newfi73', 'data/raw_frames/movies/newfi74avi', 'data/raw_frames/movies/newfi75', 'data/raw_frames/movies/newfi76', 'data/raw_frames/movies/newfi77', 'data/raw_frames/movies/newfi78', 'data/raw_frames/movies/newfi79', 'data/raw_frames/movies/newfi8', 'data/raw_frames/movies/newfi80', 'data/raw_frames/movies/newfi81', 'data/raw_frames/movies/newfi82', 'data/raw_frames/movies/newfi83', 'data/raw_frames/movies/newfi84', 'data/raw_frames/movies/newfi85', 'data/raw_frames/movies/newfi86', 'data/raw_frames/movies/newfi87', 'data/raw_frames/movies/newfi88', 'data/raw_frames/movies/newfi89', 'data/raw_frames/movies/newfi9', 'data/raw_frames/movies/newfi90', 'data/raw_frames/movies/newfi91', 'data/raw_frames/movies/newfi92', 'data/raw_frames/movies/newfi93', 'data/raw_frames/movies/newfi94', 'data/raw_frames/movies/newfi95', 'data/raw_frames/movies/newfi96', 'data/raw_frames/movies/newfi97', 'data/raw_frames/movies/newfi98', 'data/raw_frames/movies/newfi99']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdOnJUZVQ6Ga"
      },
      "source": [
        "# **Train / Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEo2N_RVQLwi",
        "outputId": "6ac850c5-10e4-485e-cea4-55de2d234f6f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#split up our data into a  training and test set\n",
        "train_path, test_path, train_y, test_y =  train_test_split(videos_frames_paths,videos_labels, test_size=0.20, random_state=42)\n",
        "\n",
        "# if apply_aug:\n",
        "#     aug_paths = []\n",
        "#     aug_y = []\n",
        "#     for train_path_, train_y_ in zip(train_path,train_y):\n",
        "#\n",
        "#         aug_path = generate_augmentations(train_path_,force = False)\n",
        "#         aug_paths.append(aug_path)\n",
        "#         aug_y.append(train_y_)\n",
        "#\n",
        "#     train_path = train_path + aug_paths\n",
        "#     train_y = train_y + aug_y\n",
        "\n",
        "train_path, valid_path, train_y, valid_y = train_test_split(train_path, train_y, test_size=0.20, random_state=42)\n",
        "print(\"Dataset splitted\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset splitted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6kpF54nhgLj"
      },
      "source": [
        "----------------------------------------------------------------------------\n",
        "# **GET SEQUENCES:** \n",
        "it makes sequence of adjacent images\n",
        "\n",
        "Requires:\n",
        "- FRAME_LOADER\n",
        "- CROP_IMG_REMOVE_DARK: removes black border background from image\n",
        "- CROP_IMG: cuts the image to a random side (Center, left up, left down, right up, right down)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6Aa7XdEhcNY"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.preprocessing import image\n",
        "import random\n",
        "corner_keys = [\"Center\",\"Left_up\",\"Left_down\",\"Right_up\",\"Right_down\"]\n",
        "\n",
        "def frame_loader(frames,figure_shape,to_norm = True):\n",
        "    output_frames = []\n",
        "    for frame in frames:\n",
        "        image = load_img(frame, target_size=(figure_shape, figure_shape),interpolation='bilinear')\n",
        "        img_arr = img_to_array(image)\n",
        "        # Scale\n",
        "        figure = (img_arr / 255.).astype(np.float32)\n",
        "        # Normalize\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "        figure = (figure - mean) / std\n",
        "        output_frames.append(figure)\n",
        "    return output_frames\n",
        "\n",
        "\n",
        "def crop_img__remove_Dark(img, x_crop,y_crop, x,y, figure_size):\n",
        "    x_start = x_crop\n",
        "    x_end = x-x_crop\n",
        "    y_start = y_crop\n",
        "    y_end = y-y_crop\n",
        "    return cv2.resize(img[y_start:y_end,x_start:x_end,:],(figure_size,figure_size))\n",
        "\n",
        "\n",
        "def crop_img(img, figure_shape, percentage=0.8, corner=\"Left_up\"):\n",
        "    if (corner == None):\n",
        "        corner = random.choice(corner_keys)\n",
        "    if corner not in corner_keys:\n",
        "        raise ValueError('Invalid corner method {} specified. Supported corners are {}'.format(corner, \", \".join(corner_keys)))\n",
        "\n",
        "    resize = int(figure_shape*percentage)\n",
        "\n",
        "    if (corner ==\"Left_up\"):\n",
        "        x_start = 0\n",
        "        x_end = resize\n",
        "        y_start = 0\n",
        "        y_end = resize\n",
        "    if (corner == \"Right_down\"):\n",
        "        x_start = figure_shape-resize\n",
        "        x_end = figure_shape\n",
        "        y_start = figure_shape-resize\n",
        "        y_end = figure_shape\n",
        "    if (corner ==\"Right_up\"):\n",
        "        x_start = 0\n",
        "        x_end = resize\n",
        "        y_start = figure_shape-resize\n",
        "        y_end = figure_shape\n",
        "    if (corner == \"Left_down\"):\n",
        "        x_start = figure_shape-resize\n",
        "        x_end = figure_shape\n",
        "        y_start = 0\n",
        "        y_end = resize\n",
        "    if (corner == \"Center\"):\n",
        "        half = int(figure_shape*(1-percentage))\n",
        "        x_start = half\n",
        "        x_end = figure_shape-half\n",
        "        y_start = half\n",
        "        y_end = figure_shape-half\n",
        "\n",
        "    img = cv2.resize(img[y_start:y_end,x_start:x_end, :], (figure_shape, figure_shape)).astype(np.float32)\n",
        "    return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8Vpeh0Fn-Wk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "93b34a0b-5daa-483e-b6ac-ec59dbc8bad2"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import scipy\n",
        "import glob\n",
        "Debug_Print_AUG=True     # True to save augmented images\n",
        "\n",
        "def get_sequences(data_paths, labels, figure_shape, seq_length, classes=1, use_augmentation = False, use_crop=True, crop_x_y=None):\n",
        "    X, y = [], []\n",
        "    seq_len = 0\n",
        "    for data_path, label in zip(data_paths,labels):\n",
        "        frames = sorted(glob.glob(os.path.join(data_path, '*jpg')))\n",
        "        x = frame_loader(frames, figure_shape)\n",
        "\n",
        "        if (crop_x_y):                                                            #remove dark from background\n",
        "            x = [crop_img__remove_Dark(x_,crop_x_y[0],crop_x_y[1],x_.shape[0],x_.shape[1],figure_shape) for x_ in x]\n",
        "        \n",
        "        if use_augmentation:                                                      #data augmentation\n",
        "            rand = scipy.random.random()\n",
        "            corner=\"\"\n",
        "            if rand > 0.5:\n",
        "                if (use_crop):\n",
        "                    corner= random.choice(corner_keys)\n",
        "                    x = [crop_img(x_,figure_shape, 0.7, corner) for x_ in x]        #crop random corner of an image\n",
        "                x = [frame.transpose(1, 0, 2) for frame in x]                     #transpose \n",
        "\n",
        "                if (Debug_Print_AUG):                                             #to save augm. image on disk\n",
        "                    to_write = [list(a) for a in zip(frames, x)]\n",
        "                    [cv2.imwrite(x_[0] + \"_\" + corner, x_[1] * 255) for x_ in to_write]\n",
        "\n",
        "        x = [x[i] - x[i+1] for i in range(len(x)-1)]                              #subtraction of adjacent frames\n",
        "        X.append(x)\n",
        "        y.append(label)\n",
        "    X = pad_sequences(X, maxlen=seq_length, padding='pre', truncating='pre')\n",
        "    \n",
        "    if classes > 1:                        #only for categorical crossentropy\n",
        "        x_ = to_categorical(x_,classes)\n",
        "    return np.array(X), np.array(y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-039f1e2ca170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mDebug_Print_AUG\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m     \u001b[0;31m# True to save augmented images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'to_categorical' from 'keras.utils' (/usr/local/lib/python3.7/dist-packages/keras/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fkod9PtewBy"
      },
      "source": [
        "# **DATA GENERATOR** \n",
        "- requires GET SEQUENCES\n",
        "\n",
        "Data generators allow to feed the model with large data, with the yield that returns part of the data gradually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eFe062khBr6"
      },
      "source": [
        "from numpy.random import shuffle\n",
        "def data_generator(data_paths, labels, batch_size, figure_shape, seq_length, use_aug, use_crop, crop_x_y, classes = 1):\n",
        "    while True:\n",
        "        indexes = np.arange(len(data_paths))\n",
        "        np.random.shuffle(indexes)\n",
        "        select_indexes = indexes[:batch_size]\n",
        "        data_paths_batch = [data_paths[i] for i in select_indexes]\n",
        "        labels_batch = [labels[i] for i in select_indexes]\n",
        "\n",
        "        X, y = get_sequences(data_paths_batch, labels_batch, figure_shape, seq_length, classes, use_augmentation = use_aug, use_crop=use_crop, crop_x_y=crop_x_y)\n",
        "\n",
        "        yield X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4hkgqu3Qfpf"
      },
      "source": [
        "crop_dark = dict(\n",
        "    hocky = (11, 38),\n",
        "    violentflow = None,\n",
        "    movies = None\n",
        ")\n",
        "\n",
        "crop_x_y = None\n",
        "if (crop_dark):   #None\n",
        "    crop_x_y = crop_dark[dataset_name]\n",
        "\n",
        "batch_size = 2                  #16\n",
        "len_train, len_valid = len(train_path), len(valid_path)       #fig size #fix len    #False\n",
        "print(len_train, len_valid, len(test_path))\n",
        "\n",
        "train_gen =    data_generator(train_path, train_y, batch_size, 244, 20, use_aug=True,  use_crop=True,  crop_x_y=crop_x_y, classes=1)\n",
        "validate_gen = data_generator(valid_path, valid_y, batch_size, 244, 20, use_aug=False, use_crop=False, crop_x_y=crop_x_y, classes=1)\n",
        "\n",
        "test_x, test_y = get_sequences(test_path, test_y, 244, 20, crop_x_y=crop_x_y, classes=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJT47wWL5yu7"
      },
      "source": [
        "# **ARCHITECTURE**\n",
        "We describe the **architecture** build upon four type of layers, \n",
        "- The **first** is the **input layer** that receive a sequence of 10 frames that are a computed difference of two adjacent frames from the original video.\n",
        "\n",
        "- The **second** type of **layers** belongs to a **Resnet50 CNN network** that aim to classify images, the initial weights of the layers are taken form a pre-trained model on **image-net**, the CNN process each frame separately and during training the weights of the network are shared. \n",
        "\n",
        "- The **third layer** is the **Convolution LSTM (ConvLSTM)** where each frame from the CNN enters into a **ConvLSTM** cell with an hidden state of **256 convolution filters** of **size 3**. \n",
        "\n",
        "- The **forth** type of **layers** process the ConvLSTM and **output** the **binary prediction**, a **Max pooling layer** of **size 2** reduces the data and chooses the most informational pixels, then the data is **batch normalized** and connected to a series of **fully connected layer** of sizes **1000, 256, 10** and finally a **binary output perception** with a **sigmoid activation function**. \n",
        "- **Between** each of the **fully connected layers** we use** RELU activation**.\n",
        "\n",
        "- We use **binary cross entropy** as our **loss function** and **RMSprop** as an **optimizer**, **20%** of the data is select for **validation** and rest **80%** is selected to **train**. \n",
        "\n",
        "- The **learning rate** of the network **starts** with value of **0.0001** and is **reduced** by **half** **after 5 epochs** of **no improvement** in the **validation loss**. \n",
        "\n",
        "- We train the model with **50 epochs** but also use **early stopping** in case where the **network validation loss haven't improve for 15 epochs.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnCSPzD5w4Jh"
      },
      "source": [
        "model fit: # the network is trained on data generatores and apply the callacks when the validation loss is not improving:\n",
        "- 1) early stop to training after n iteration\n",
        "- 2) reducing the learning rate after k iteration where k< n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np600e_tZsvB"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import Input\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "from keras.layers import Dense, Flatten, Dropout, ConvLSTM2D, BatchNormalization, Activation, Reshape, LSTM, GlobalAveragePooling1D\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "from keras.layers.convolutional import (MaxPooling2D)\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from keras import backend as K    #added\n",
        "K.clear_session()                 #added\n",
        "\n",
        "def model1():\n",
        "  num_classes = 2\n",
        "  input_shapes=(160,160,3)\n",
        "  np.random.seed(1234)\n",
        "\n",
        "  base_model = VGG19(include_top=False, weights='imagenet', input_shape=(160, 160,3))\n",
        "  #for layer in base_model.layers:    # Freeze the layers except the last 4 layers\n",
        "  #    layer.trainable = False\n",
        "\n",
        "  cnn = Sequential()\n",
        "  cnn.add(base_model)\n",
        "  cnn.add(Flatten())\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(TimeDistributed(cnn,  input_shape=(30, 160, 160, 3)))\n",
        "  model.add(LSTM(30 , return_sequences= True))\n",
        "  model.add(TimeDistributed(Dense(90)))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(GlobalAveragePooling1D())\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(num_classes, activation=\"sigmoid\"))\n",
        "\n",
        "  adam = Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "  #model.load_weights(w)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "def model2():\n",
        "  num_classes = 1\n",
        "  tf.random.set_seed(2)\n",
        "  np.random.seed(1)\n",
        "\n",
        "  base_model = ResNet50(include_top=False, weights='imagenet' , input_shape=(244, 244, 3))\n",
        "  for layer in base_model.layers:      #retrain\n",
        "      layer.trainable = True           #False if static\n",
        "\n",
        "  cnn = Sequential()\n",
        "  cnn.add(base_model)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(TimeDistributed(cnn, input_shape=(20, 244, 244, 3)))\n",
        "  model.add(ConvLSTM2D(filters=256, kernel_size=(3, 3), padding='same', return_sequences=False))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Dense(1000, activation='relu'))\n",
        "  #model.add(Dropout(0.0))\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  #model.add(Dropout(0.0))\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  #model.add(Dropout(0.0))\n",
        "  model.add(Dense(num_classes , activation='sigmoid'))             #'softmax' if classes > 1\n",
        "\n",
        "  #adam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08)  #Adam, 1e-4, 1e-3 \n",
        "  rms =  RMSprop(learning_rate = 0.0001)\n",
        "  model.compile(loss= 'binary_crossentropy', optimizer= rms, metrics=[\"accuracy\"])           #'categorical_crossentropy' if classes >1\n",
        "  return model\n",
        "\n",
        "model = model2()\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60zeTRl3c_9Z"
      },
      "source": [
        "hist = model.fit(train_gen, validation_data = validate_gen,\n",
        "    steps_per_epoch= int(float(len_train) / float(batch_size * 0.5)),   #batch_epoch_ratio = 0.5\n",
        "    epochs = 50,                   \n",
        "    validation_steps = int(float(len_valid) / float(batch_size)),\n",
        "    callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.001, patience=15, ),\n",
        "                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5 , min_lr=1e-8, verbose=1)]\n",
        "                #TestCallback((test_x, test_y))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekrjyNAL5-3v"
      },
      "source": [
        "# **EVALUATION**\n",
        "- The best performing **CNN** is the **Resnet50** with **90% accuracy**, the InceptionV3 CNN was not far from the Resnet50 with 89% accuracy but the VGG19 CNN had poor results of only 79% accuracy.\n",
        "- The **starting learning rate** value had a critical effect on the network results where the 0.001 learning rate resolved with only 46% accuracy which is lower then the random classiffcation. \n",
        "as already mentioned by the original paper, the **learning rate** of **0.0001** had far better results in all experiments.\n",
        "- The **augmentation** increases the accuracy by** 4.5%** and smaller length size of the **sequence** improve the accuracy by **2%.** \n",
        "the dropout of 50% did no improve the model performance and results with only 86% accuracy. \n",
        "- As expected the static CNN configuration where the CNN weights are not retrained had very poor results of 59% accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7EdHY6-5X-G"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "score = model.evaluate(test_x, test_y, batch_size=2)\n",
        "print(model.metrics)\n",
        "print('Loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "########################### ACCURACY ###################################################\n",
        "plt.plot(hist.history['accuracy'], label='train')\n",
        "plt.plot(hist.history['val_accuracy'], label='val')\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "##################### LOSS ###############################\n",
        "plt.plot(hist.history['loss'], label='train')\n",
        "plt.plot(hist.history['val_loss'], label='val')\n",
        "#plt.title('model loss')\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhMmPT6FUudS"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion(y_true, y_pred):\n",
        "  conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "  plt.imshow(conf_matrix)\n",
        "  plt.xticks([0, 1], ['NonViolence', 'Violence'], fontsize=16)\n",
        "  plt.yticks([0, 1], ['NonViolence', 'Violence'], fontsize=16)\n",
        "  plt.ylabel('Truth', fontsize=20)\n",
        "  plt.xlabel('Prediction', fontsize=20)\n",
        "  plt.colorbar()\n",
        "\n",
        "  plt.grid(False)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ircs3NMKtbVH"
      },
      "source": [
        "prediction_probabilities = (model.predict(test_x, batch_size=2).ravel()>0.5)+0\n",
        "print(prediction_probabilities)\n",
        "print(test_y)\n",
        "plot_confusion(test_y, prediction_probabilities)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}