{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copia_2_di_PROVE_ViolenceDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esPqye6MMEs4"
      },
      "source": [
        "# **VIOLENCE DETECTION V1.0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTT2x9YGnY9T"
      },
      "source": [
        "----------------------------------------------------------------\n",
        "# **CONNECTION** (google drive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYIPv6fOmomh",
        "outputId": "06ba9566-92b6-41ea-9405-6255cd3fbd1f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/My Drive\n",
            "'Colab Notebooks'   data\t     filesFilmando2.rar   GPUs.gsheet\n",
            " CUSTOM.gsheet\t    filesFilmando2   filmando-eng.rar\t  lib.rar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-idtdsGKbYO"
      },
      "source": [
        "# **DATA PREPROCESSING**\n",
        "As a **preparation** for the **graph input** few steps were taken in the dataset preparation\n",
        "- initially the **videos** were **sampled** to a frame by **frame sequence** as we were limited with computational power. \n",
        "- The videos were sampled into a **fix number** of **frames** before given as an input to the model. \n",
        "\n",
        "- For all dataset combination of **augmentation methods** were used and for some of the datasets, **dark edges** were **removed** from the frame as we present in Figure 3.\n",
        "- As the original article stated, the **input** to the model is a **subtraction** of **adjacent frames**, this was done in order to include a **spatial movements** in the input videos instead of the raw pixels from each frame. \n",
        "- In Figure 2 we present and example of difference computation of adjacent frames where an hockey player pushes another player.\n",
        "\n",
        "-To enrich and **enlarge** the **dataset** we apply **data augmentation** with the following\n",
        "**transformations** on the frames:\n",
        "  - **Image cropping**: a **slicing** of the **image**, done each time with a **different** anchor **corner** was chosen (Figure 4) .\n",
        "  - **Image transpose**: as a complement steps to the cropping process, a transpose was done, this **step** was done during the **fit generator process** (Figure 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RefAlxwLdlrp"
      },
      "source": [
        "**SAVE_FIGURES_FROM_VIDEO** (required for CREATE DATASET)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvFG8rxIoS_1"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "def save_figures_from_video(dataset_video_path, video_filename, suffix,figures_path,skip_frames = 25,apply_norm = True, apply_diff = True,fix_len = None):\n",
        "    seq_len = 0\n",
        "\n",
        "    video_figures_path = os.path.join(figures_path ,video_filename)\n",
        "    if not os.path.exists(video_figures_path):\n",
        "        os.makedirs(video_figures_path)\n",
        "\n",
        "    video_file = os.path.join(dataset_video_path, video_filename + suffix)\n",
        "    label = 0\n",
        "    #print('Extracting frames from video: ', video_file)\n",
        "    print('.', end='')\n",
        "\n",
        "    videoCapture = cv2.VideoCapture(video_file)\n",
        "    if fix_len is not None:\n",
        "        vid_len = int(videoCapture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        skip_frames = int(float(vid_len)/float(fix_len))\n",
        "    videoCapture.set(cv2.CAP_PROP_POS_MSEC, (seq_len * skip_frames))\n",
        "    success, figure_ = videoCapture.read()\n",
        "    success = True\n",
        "    files = []\n",
        "    while success:\n",
        "        success, figure = videoCapture.read()\n",
        "\n",
        "        if seq_len % skip_frames == 0:\n",
        "            if success:\n",
        "                figure_curr = figure\n",
        "                image_file = os.path.join(video_figures_path , \"frame_%d.jpg\" % seq_len)\n",
        "                files.append(image_file)\n",
        "                cv2.imwrite(image_file, figure_curr)\n",
        "        seq_len += 1\n",
        "    video_images = dict(images_path = video_figures_path, name = video_filename,\n",
        "                        images_files = files, sequence_length = seq_len, label = label)\n",
        "\n",
        "    return video_images\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuABj-PO5FNZ",
        "outputId": "3e6456dc-2b73-41d0-c988-36e7e6af0295"
      },
      "source": [
        "import pickle\n",
        "\n",
        "datasets_videos = dict(\n",
        "    hocky = dict(hocky=\"data/raw_videos/HockeyFights\"),\n",
        "    violentflow = dict(violentflow=\"data/raw_videos/violentflow\"),\n",
        "    movies = dict(movies=\"data/raw_videos/movies\")\n",
        ")\n",
        "\n",
        "#for dataset_name, dataset_videos in datasets_videos.items():\n",
        "dataset_name = \"hocky\"                          #         \"movies\"  \"violentflow\"   \"hocky\"\n",
        "dataset_videos = datasets_videos.get(\"hocky\")   #         \"movies\"  \"violentflow\"   \"hocky\"\n",
        "print(dataset_name, dataset_videos)\n",
        "\n",
        "force=False           #True to force rebuilding dataset\n",
        "videos_seq_length = []\n",
        "datasets_images = {}\n",
        "videos_frames_paths = []\n",
        "videos_labels = []\n",
        "datasets_video_path = dataset_videos\n",
        "figure_output_path = \"data/raw_frames\"\n",
        "\n",
        "#Extract images for each video for the dataset\n",
        "for dataset_name, dataset_video_path in datasets_video_path.items():\n",
        "    dataset_figures_path = os.path.join(figure_output_path,dataset_name)\n",
        "    if not os.path.exists(dataset_figures_path):\n",
        "        os.makedirs(dataset_figures_path)\n",
        "    dataset_images = []\n",
        "    \n",
        "    for filename in os.listdir(dataset_video_path):\n",
        "        if filename.endswith(\".avi\") or filename.endswith(\".mpg\"):\n",
        "            video_images_file = os.path.join(dataset_figures_path,filename[:-4], 'video_summary.pkl')\n",
        "            if os.path.isfile(video_images_file) and not force:\n",
        "                with open(video_images_file, 'rb') as f:\n",
        "                    video_images = pickle.load(f)               #load dump of frames already decomposed\n",
        "            else:\n",
        "                video_images = save_figures_from_video(dataset_video_path, filename[:-4],filename[-4:], dataset_figures_path, fix_len = 20)\n",
        "                if dataset_name == \"hocky\":\n",
        "                    if filename.startswith(\"fi\"):\n",
        "                        video_images['label'] = 1\n",
        "                elif dataset_name == \"violentflow\":\n",
        "                    if \"violence\" in filename:\n",
        "                        video_images['label'] = 1\n",
        "                elif dataset_name == \"movies\":\n",
        "                    if \"fi\" in filename:\n",
        "                        video_images['label'] = 1\n",
        "                with open(video_images_file, 'wb') as f:\n",
        "                    pickle.dump(video_images, f, pickle.HIGHEST_PROTOCOL)\n",
        "            dataset_images.append(video_images)\n",
        "            videos_seq_length.append(video_images['sequence_length'])\n",
        "            videos_frames_paths.append(video_images['images_path'])\n",
        "            videos_labels.append(video_images['label'])\n",
        "    datasets_images[dataset_name] = dataset_images\n",
        "avg_length = int(float(sum(videos_seq_length)) / max(len(videos_seq_length), 1))\n",
        "\n",
        "#print(videos_seq_length)\n",
        "print(videos_frames_paths)\n",
        "print(videos_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hocky {'hocky': 'data/raw_videos/HockeyFights'}\n",
            "['data/raw_frames/hocky/fi100_xvid', 'data/raw_frames/hocky/fi102_xvid', 'data/raw_frames/hocky/fi101_xvid', 'data/raw_frames/hocky/fi103_xvid', 'data/raw_frames/hocky/fi104_xvid', 'data/raw_frames/hocky/fi106_xvid', 'data/raw_frames/hocky/fi108_xvid', 'data/raw_frames/hocky/fi107_xvid', 'data/raw_frames/hocky/fi105_xvid', 'data/raw_frames/hocky/fi109_xvid', 'data/raw_frames/hocky/fi10_xvid', 'data/raw_frames/hocky/fi111_xvid', 'data/raw_frames/hocky/fi112_xvid', 'data/raw_frames/hocky/fi110_xvid', 'data/raw_frames/hocky/fi113_xvid', 'data/raw_frames/hocky/fi114_xvid', 'data/raw_frames/hocky/fi115_xvid', 'data/raw_frames/hocky/fi116_xvid', 'data/raw_frames/hocky/fi117_xvid', 'data/raw_frames/hocky/fi118_xvid', 'data/raw_frames/hocky/fi119_xvid', 'data/raw_frames/hocky/fi11_xvid', 'data/raw_frames/hocky/fi120_xvid', 'data/raw_frames/hocky/fi121_xvid', 'data/raw_frames/hocky/fi123_xvid', 'data/raw_frames/hocky/fi122_xvid', 'data/raw_frames/hocky/fi124_xvid', 'data/raw_frames/hocky/fi126_xvid', 'data/raw_frames/hocky/fi125_xvid', 'data/raw_frames/hocky/fi127_xvid', 'data/raw_frames/hocky/fi128_xvid', 'data/raw_frames/hocky/fi129_xvid', 'data/raw_frames/hocky/fi12_xvid', 'data/raw_frames/hocky/fi130_xvid', 'data/raw_frames/hocky/fi131_xvid', 'data/raw_frames/hocky/fi132_xvid', 'data/raw_frames/hocky/fi134_xvid', 'data/raw_frames/hocky/fi133_xvid', 'data/raw_frames/hocky/fi135_xvid', 'data/raw_frames/hocky/fi136_xvid', 'data/raw_frames/hocky/fi137_xvid', 'data/raw_frames/hocky/fi138_xvid', 'data/raw_frames/hocky/fi139_xvid', 'data/raw_frames/hocky/fi13_xvid', 'data/raw_frames/hocky/fi140_xvid', 'data/raw_frames/hocky/fi141_xvid', 'data/raw_frames/hocky/fi142_xvid', 'data/raw_frames/hocky/fi143_xvid', 'data/raw_frames/hocky/fi144_xvid', 'data/raw_frames/hocky/fi145_xvid', 'data/raw_frames/hocky/fi146_xvid', 'data/raw_frames/hocky/fi147_xvid', 'data/raw_frames/hocky/fi148_xvid', 'data/raw_frames/hocky/fi14_xvid', 'data/raw_frames/hocky/fi149_xvid', 'data/raw_frames/hocky/fi150_xvid', 'data/raw_frames/hocky/fi151_xvid', 'data/raw_frames/hocky/fi152_xvid', 'data/raw_frames/hocky/fi153_xvid', 'data/raw_frames/hocky/fi154_xvid', 'data/raw_frames/hocky/fi155_xvid', 'data/raw_frames/hocky/fi156_xvid', 'data/raw_frames/hocky/fi157_xvid', 'data/raw_frames/hocky/fi158_xvid', 'data/raw_frames/hocky/fi159_xvid', 'data/raw_frames/hocky/fi15_xvid', 'data/raw_frames/hocky/fi160_xvid', 'data/raw_frames/hocky/fi161_xvid', 'data/raw_frames/hocky/fi162_xvid', 'data/raw_frames/hocky/fi163_xvid', 'data/raw_frames/hocky/fi164_xvid', 'data/raw_frames/hocky/fi165_xvid', 'data/raw_frames/hocky/fi166_xvid', 'data/raw_frames/hocky/fi167_xvid', 'data/raw_frames/hocky/fi168_xvid', 'data/raw_frames/hocky/fi16_xvid', 'data/raw_frames/hocky/fi169_xvid', 'data/raw_frames/hocky/fi170_xvid', 'data/raw_frames/hocky/fi171_xvid', 'data/raw_frames/hocky/fi172_xvid', 'data/raw_frames/hocky/fi173_xvid', 'data/raw_frames/hocky/fi174_xvid', 'data/raw_frames/hocky/fi175_xvid', 'data/raw_frames/hocky/fi176_xvid', 'data/raw_frames/hocky/fi177_xvid', 'data/raw_frames/hocky/fi178_xvid', 'data/raw_frames/hocky/fi179_xvid', 'data/raw_frames/hocky/fi17_xvid', 'data/raw_frames/hocky/fi180_xvid', 'data/raw_frames/hocky/fi181_xvid', 'data/raw_frames/hocky/fi182_xvid', 'data/raw_frames/hocky/fi183_xvid', 'data/raw_frames/hocky/fi184_xvid', 'data/raw_frames/hocky/fi185_xvid', 'data/raw_frames/hocky/fi186_xvid', 'data/raw_frames/hocky/fi188_xvid', 'data/raw_frames/hocky/fi187_xvid', 'data/raw_frames/hocky/fi189_xvid', 'data/raw_frames/hocky/fi18_xvid', 'data/raw_frames/hocky/fi190_xvid', 'data/raw_frames/hocky/fi191_xvid', 'data/raw_frames/hocky/fi192_xvid', 'data/raw_frames/hocky/fi194_xvid', 'data/raw_frames/hocky/fi193_xvid', 'data/raw_frames/hocky/fi195_xvid', 'data/raw_frames/hocky/fi197_xvid', 'data/raw_frames/hocky/fi196_xvid', 'data/raw_frames/hocky/fi198_xvid', 'data/raw_frames/hocky/fi199_xvid', 'data/raw_frames/hocky/fi19_xvid', 'data/raw_frames/hocky/fi1_xvid', 'data/raw_frames/hocky/fi200_xvid', 'data/raw_frames/hocky/fi201_xvid', 'data/raw_frames/hocky/fi202_xvid', 'data/raw_frames/hocky/fi203_xvid', 'data/raw_frames/hocky/fi205_xvid', 'data/raw_frames/hocky/fi204_xvid', 'data/raw_frames/hocky/fi206_xvid', 'data/raw_frames/hocky/fi207_xvid', 'data/raw_frames/hocky/fi208_xvid', 'data/raw_frames/hocky/fi209_xvid', 'data/raw_frames/hocky/fi20_xvid', 'data/raw_frames/hocky/fi210_xvid', 'data/raw_frames/hocky/fi211_xvid', 'data/raw_frames/hocky/fi212_xvid', 'data/raw_frames/hocky/fi213_xvid', 'data/raw_frames/hocky/fi214_xvid', 'data/raw_frames/hocky/fi215_xvid', 'data/raw_frames/hocky/fi217_xvid', 'data/raw_frames/hocky/fi216_xvid', 'data/raw_frames/hocky/fi218_xvid', 'data/raw_frames/hocky/fi21_xvid', 'data/raw_frames/hocky/fi219_xvid', 'data/raw_frames/hocky/fi220_xvid', 'data/raw_frames/hocky/fi222_xvid', 'data/raw_frames/hocky/fi221_xvid', 'data/raw_frames/hocky/fi223_xvid', 'data/raw_frames/hocky/fi224_xvid', 'data/raw_frames/hocky/fi225_xvid', 'data/raw_frames/hocky/fi226_xvid', 'data/raw_frames/hocky/fi227_xvid', 'data/raw_frames/hocky/fi228_xvid', 'data/raw_frames/hocky/fi229_xvid', 'data/raw_frames/hocky/fi22_xvid', 'data/raw_frames/hocky/fi230_xvid', 'data/raw_frames/hocky/fi231_xvid', 'data/raw_frames/hocky/fi233_xvid', 'data/raw_frames/hocky/fi232_xvid', 'data/raw_frames/hocky/fi234_xvid', 'data/raw_frames/hocky/fi235_xvid', 'data/raw_frames/hocky/fi236_xvid', 'data/raw_frames/hocky/fi237_xvid', 'data/raw_frames/hocky/fi238_xvid', 'data/raw_frames/hocky/fi239_xvid', 'data/raw_frames/hocky/fi23_xvid', 'data/raw_frames/hocky/fi240_xvid', 'data/raw_frames/hocky/fi241_xvid', 'data/raw_frames/hocky/fi243_xvid', 'data/raw_frames/hocky/fi244_xvid', 'data/raw_frames/hocky/fi242_xvid', 'data/raw_frames/hocky/fi246_xvid', 'data/raw_frames/hocky/fi245_xvid', 'data/raw_frames/hocky/fi247_xvid', 'data/raw_frames/hocky/fi249_xvid', 'data/raw_frames/hocky/fi248_xvid', 'data/raw_frames/hocky/fi24_xvid', 'data/raw_frames/hocky/fi251_xvid', 'data/raw_frames/hocky/fi250_xvid', 'data/raw_frames/hocky/fi252_xvid', 'data/raw_frames/hocky/fi253_xvid', 'data/raw_frames/hocky/fi255_xvid', 'data/raw_frames/hocky/fi254_xvid', 'data/raw_frames/hocky/fi256_xvid', 'data/raw_frames/hocky/fi257_xvid', 'data/raw_frames/hocky/fi258_xvid', 'data/raw_frames/hocky/fi259_xvid', 'data/raw_frames/hocky/fi25_xvid', 'data/raw_frames/hocky/fi261_xvid', 'data/raw_frames/hocky/fi260_xvid', 'data/raw_frames/hocky/fi262_xvid', 'data/raw_frames/hocky/fi263_xvid', 'data/raw_frames/hocky/fi264_xvid', 'data/raw_frames/hocky/fi266_xvid', 'data/raw_frames/hocky/fi265_xvid', 'data/raw_frames/hocky/fi267_xvid', 'data/raw_frames/hocky/fi268_xvid', 'data/raw_frames/hocky/fi269_xvid', 'data/raw_frames/hocky/fi26_xvid', 'data/raw_frames/hocky/fi270_xvid', 'data/raw_frames/hocky/fi271_xvid', 'data/raw_frames/hocky/fi272_xvid', 'data/raw_frames/hocky/fi273_xvid', 'data/raw_frames/hocky/fi274_xvid', 'data/raw_frames/hocky/fi275_xvid', 'data/raw_frames/hocky/fi277_xvid', 'data/raw_frames/hocky/fi276_xvid', 'data/raw_frames/hocky/fi278_xvid', 'data/raw_frames/hocky/fi27_xvid', 'data/raw_frames/hocky/fi279_xvid', 'data/raw_frames/hocky/fi280_xvid', 'data/raw_frames/hocky/fi281_xvid', 'data/raw_frames/hocky/fi283_xvid', 'data/raw_frames/hocky/fi282_xvid', 'data/raw_frames/hocky/fi284_xvid', 'data/raw_frames/hocky/fi286_xvid', 'data/raw_frames/hocky/fi285_xvid', 'data/raw_frames/hocky/fi287_xvid', 'data/raw_frames/hocky/fi288_xvid', 'data/raw_frames/hocky/fi289_xvid', 'data/raw_frames/hocky/fi28_xvid', 'data/raw_frames/hocky/fi291_xvid', 'data/raw_frames/hocky/fi290_xvid', 'data/raw_frames/hocky/fi292_xvid', 'data/raw_frames/hocky/fi293_xvid', 'data/raw_frames/hocky/fi294_xvid', 'data/raw_frames/hocky/fi295_xvid', 'data/raw_frames/hocky/fi296_xvid', 'data/raw_frames/hocky/fi297_xvid', 'data/raw_frames/hocky/fi298_xvid', 'data/raw_frames/hocky/fi299_xvid', 'data/raw_frames/hocky/fi29_xvid', 'data/raw_frames/hocky/fi2_xvid', 'data/raw_frames/hocky/fi301_xvid', 'data/raw_frames/hocky/fi302_xvid', 'data/raw_frames/hocky/fi303_xvid', 'data/raw_frames/hocky/fi304_xvid', 'data/raw_frames/hocky/fi305_xvid', 'data/raw_frames/hocky/fi307_xvid', 'data/raw_frames/hocky/fi306_xvid', 'data/raw_frames/hocky/fi300_xvid', 'data/raw_frames/hocky/fi308_xvid', 'data/raw_frames/hocky/fi309_xvid', 'data/raw_frames/hocky/fi310_xvid', 'data/raw_frames/hocky/fi312_xvid', 'data/raw_frames/hocky/fi30_xvid', 'data/raw_frames/hocky/fi311_xvid', 'data/raw_frames/hocky/fi313_xvid', 'data/raw_frames/hocky/fi314_xvid', 'data/raw_frames/hocky/fi316_xvid', 'data/raw_frames/hocky/fi315_xvid', 'data/raw_frames/hocky/fi318_xvid', 'data/raw_frames/hocky/fi317_xvid', 'data/raw_frames/hocky/fi319_xvid', 'data/raw_frames/hocky/fi31_xvid', 'data/raw_frames/hocky/fi320_xvid', 'data/raw_frames/hocky/fi321_xvid', 'data/raw_frames/hocky/fi324_xvid', 'data/raw_frames/hocky/fi322_xvid', 'data/raw_frames/hocky/fi323_xvid', 'data/raw_frames/hocky/fi325_xvid', 'data/raw_frames/hocky/fi326_xvid', 'data/raw_frames/hocky/fi327_xvid', 'data/raw_frames/hocky/fi328_xvid', 'data/raw_frames/hocky/fi329_xvid', 'data/raw_frames/hocky/fi32_xvid', 'data/raw_frames/hocky/fi330_xvid', 'data/raw_frames/hocky/fi331_xvid', 'data/raw_frames/hocky/fi333_xvid', 'data/raw_frames/hocky/fi332_xvid', 'data/raw_frames/hocky/fi334_xvid', 'data/raw_frames/hocky/fi336_xvid', 'data/raw_frames/hocky/fi335_xvid', 'data/raw_frames/hocky/fi337_xvid', 'data/raw_frames/hocky/fi338_xvid', 'data/raw_frames/hocky/fi339_xvid', 'data/raw_frames/hocky/fi33_xvid', 'data/raw_frames/hocky/fi340_xvid', 'data/raw_frames/hocky/fi341_xvid', 'data/raw_frames/hocky/fi343_xvid', 'data/raw_frames/hocky/fi342_xvid', 'data/raw_frames/hocky/fi344_xvid', 'data/raw_frames/hocky/fi345_xvid', 'data/raw_frames/hocky/fi348_xvid', 'data/raw_frames/hocky/fi347_xvid', 'data/raw_frames/hocky/fi346_xvid', 'data/raw_frames/hocky/fi34_xvid', 'data/raw_frames/hocky/fi350_xvid', 'data/raw_frames/hocky/fi351_xvid', 'data/raw_frames/hocky/fi352_xvid', 'data/raw_frames/hocky/fi349_xvid', 'data/raw_frames/hocky/fi353_xvid', 'data/raw_frames/hocky/fi354_xvid', 'data/raw_frames/hocky/fi355_xvid', 'data/raw_frames/hocky/fi356_xvid', 'data/raw_frames/hocky/fi357_xvid', 'data/raw_frames/hocky/fi359_xvid', 'data/raw_frames/hocky/fi358_xvid', 'data/raw_frames/hocky/fi361_xvid', 'data/raw_frames/hocky/fi35_xvid', 'data/raw_frames/hocky/fi362_xvid', 'data/raw_frames/hocky/fi360_xvid', 'data/raw_frames/hocky/fi363_xvid', 'data/raw_frames/hocky/fi364_xvid', 'data/raw_frames/hocky/fi365_xvid', 'data/raw_frames/hocky/fi368_xvid', 'data/raw_frames/hocky/fi366_xvid', 'data/raw_frames/hocky/fi367_xvid', 'data/raw_frames/hocky/fi369_xvid', 'data/raw_frames/hocky/fi373_xvid', 'data/raw_frames/hocky/fi36_xvid', 'data/raw_frames/hocky/fi370_xvid', 'data/raw_frames/hocky/fi372_xvid', 'data/raw_frames/hocky/fi371_xvid', 'data/raw_frames/hocky/fi375_xvid', 'data/raw_frames/hocky/fi376_xvid', 'data/raw_frames/hocky/fi374_xvid', 'data/raw_frames/hocky/fi377_xvid', 'data/raw_frames/hocky/fi378_xvid', 'data/raw_frames/hocky/fi379_xvid', 'data/raw_frames/hocky/fi37_xvid', 'data/raw_frames/hocky/fi381_xvid', 'data/raw_frames/hocky/fi380_xvid', 'data/raw_frames/hocky/fi383_xvid', 'data/raw_frames/hocky/fi385_xvid', 'data/raw_frames/hocky/fi382_xvid', 'data/raw_frames/hocky/fi384_xvid', 'data/raw_frames/hocky/fi386_xvid', 'data/raw_frames/hocky/fi388_xvid', 'data/raw_frames/hocky/fi387_xvid', 'data/raw_frames/hocky/fi389_xvid', 'data/raw_frames/hocky/fi38_xvid', 'data/raw_frames/hocky/fi390_xvid', 'data/raw_frames/hocky/fi391_xvid', 'data/raw_frames/hocky/fi392_xvid', 'data/raw_frames/hocky/fi394_xvid', 'data/raw_frames/hocky/fi393_xvid', 'data/raw_frames/hocky/fi395_xvid', 'data/raw_frames/hocky/fi396_xvid', 'data/raw_frames/hocky/fi397_xvid', 'data/raw_frames/hocky/fi398_xvid', 'data/raw_frames/hocky/fi3_xvid', 'data/raw_frames/hocky/fi399_xvid', 'data/raw_frames/hocky/fi400_xvid', 'data/raw_frames/hocky/fi39_xvid', 'data/raw_frames/hocky/fi401_xvid', 'data/raw_frames/hocky/fi402_xvid', 'data/raw_frames/hocky/fi404_xvid', 'data/raw_frames/hocky/fi403_xvid', 'data/raw_frames/hocky/fi405_xvid', 'data/raw_frames/hocky/fi406_xvid', 'data/raw_frames/hocky/fi407_xvid', 'data/raw_frames/hocky/fi408_xvid', 'data/raw_frames/hocky/fi409_xvid', 'data/raw_frames/hocky/fi40_xvid', 'data/raw_frames/hocky/fi410_xvid', 'data/raw_frames/hocky/fi411_xvid', 'data/raw_frames/hocky/fi412_xvid', 'data/raw_frames/hocky/fi413_xvid', 'data/raw_frames/hocky/fi417_xvid', 'data/raw_frames/hocky/fi414_xvid', 'data/raw_frames/hocky/fi415_xvid', 'data/raw_frames/hocky/fi416_xvid', 'data/raw_frames/hocky/fi419_xvid', 'data/raw_frames/hocky/fi418_xvid', 'data/raw_frames/hocky/fi41_xvid', 'data/raw_frames/hocky/fi420_xvid', 'data/raw_frames/hocky/fi421_xvid', 'data/raw_frames/hocky/fi422_xvid', 'data/raw_frames/hocky/fi423_xvid', 'data/raw_frames/hocky/fi424_xvid', 'data/raw_frames/hocky/fi427_xvid', 'data/raw_frames/hocky/fi426_xvid', 'data/raw_frames/hocky/fi425_xvid', 'data/raw_frames/hocky/fi428_xvid', 'data/raw_frames/hocky/fi429_xvid', 'data/raw_frames/hocky/fi42_xvid', 'data/raw_frames/hocky/fi431_xvid', 'data/raw_frames/hocky/fi430_xvid', 'data/raw_frames/hocky/fi433_xvid', 'data/raw_frames/hocky/fi432_xvid', 'data/raw_frames/hocky/fi435_xvid', 'data/raw_frames/hocky/fi434_xvid', 'data/raw_frames/hocky/fi437_xvid', 'data/raw_frames/hocky/fi436_xvid', 'data/raw_frames/hocky/fi438_xvid', 'data/raw_frames/hocky/fi439_xvid', 'data/raw_frames/hocky/fi43_xvid', 'data/raw_frames/hocky/fi440_xvid', 'data/raw_frames/hocky/fi441_xvid', 'data/raw_frames/hocky/fi442_xvid', 'data/raw_frames/hocky/fi443_xvid', 'data/raw_frames/hocky/fi444_xvid', 'data/raw_frames/hocky/fi446_xvid', 'data/raw_frames/hocky/fi448_xvid', 'data/raw_frames/hocky/fi445_xvid', 'data/raw_frames/hocky/fi447_xvid', 'data/raw_frames/hocky/fi449_xvid', 'data/raw_frames/hocky/fi44_xvid', 'data/raw_frames/hocky/fi453_xvid', 'data/raw_frames/hocky/fi450_xvid', 'data/raw_frames/hocky/fi451_xvid', 'data/raw_frames/hocky/fi452_xvid', 'data/raw_frames/hocky/fi454_xvid', 'data/raw_frames/hocky/fi457_xvid', 'data/raw_frames/hocky/fi456_xvid', 'data/raw_frames/hocky/fi455_xvid', 'data/raw_frames/hocky/fi458_xvid', 'data/raw_frames/hocky/fi459_xvid', 'data/raw_frames/hocky/fi45_xvid', 'data/raw_frames/hocky/fi460_xvid', 'data/raw_frames/hocky/fi461_xvid', 'data/raw_frames/hocky/fi462_xvid', 'data/raw_frames/hocky/fi463_xvid', 'data/raw_frames/hocky/fi464_xvid', 'data/raw_frames/hocky/fi465_xvid', 'data/raw_frames/hocky/fi467_xvid', 'data/raw_frames/hocky/fi466_xvid', 'data/raw_frames/hocky/fi468_xvid', 'data/raw_frames/hocky/fi469_xvid', 'data/raw_frames/hocky/fi46_xvid', 'data/raw_frames/hocky/fi472_xvid', 'data/raw_frames/hocky/fi471_xvid', 'data/raw_frames/hocky/fi470_xvid', 'data/raw_frames/hocky/fi473_xvid', 'data/raw_frames/hocky/fi474_xvid', 'data/raw_frames/hocky/fi475_xvid', 'data/raw_frames/hocky/fi476_xvid', 'data/raw_frames/hocky/fi477_xvid', 'data/raw_frames/hocky/fi478_xvid', 'data/raw_frames/hocky/fi479_xvid', 'data/raw_frames/hocky/fi47_xvid', 'data/raw_frames/hocky/fi480_xvid', 'data/raw_frames/hocky/fi481_xvid', 'data/raw_frames/hocky/fi482_xvid', 'data/raw_frames/hocky/fi483_xvid', 'data/raw_frames/hocky/fi484_xvid', 'data/raw_frames/hocky/fi485_xvid', 'data/raw_frames/hocky/fi486_xvid', 'data/raw_frames/hocky/fi488_xvid', 'data/raw_frames/hocky/fi487_xvid', 'data/raw_frames/hocky/fi489_xvid', 'data/raw_frames/hocky/fi490_xvid', 'data/raw_frames/hocky/fi48_xvid', 'data/raw_frames/hocky/fi491_xvid', 'data/raw_frames/hocky/fi492_xvid', 'data/raw_frames/hocky/fi493_xvid', 'data/raw_frames/hocky/fi495_xvid', 'data/raw_frames/hocky/fi494_xvid', 'data/raw_frames/hocky/fi496_xvid', 'data/raw_frames/hocky/fi497_xvid', 'data/raw_frames/hocky/fi498_xvid', 'data/raw_frames/hocky/fi499_xvid', 'data/raw_frames/hocky/fi49_xvid', 'data/raw_frames/hocky/fi50_xvid', 'data/raw_frames/hocky/fi500_xvid', 'data/raw_frames/hocky/fi4_xvid', 'data/raw_frames/hocky/fi53_xvid', 'data/raw_frames/hocky/fi51_xvid', 'data/raw_frames/hocky/fi54_xvid', 'data/raw_frames/hocky/fi55_xvid', 'data/raw_frames/hocky/fi52_xvid', 'data/raw_frames/hocky/fi57_xvid', 'data/raw_frames/hocky/fi59_xvid', 'data/raw_frames/hocky/fi56_xvid', 'data/raw_frames/hocky/fi5_xvid', 'data/raw_frames/hocky/fi58_xvid', 'data/raw_frames/hocky/fi60_xvid', 'data/raw_frames/hocky/fi62_xvid', 'data/raw_frames/hocky/fi61_xvid', 'data/raw_frames/hocky/fi64_xvid', 'data/raw_frames/hocky/fi63_xvid', 'data/raw_frames/hocky/fi65_xvid', 'data/raw_frames/hocky/fi66_xvid', 'data/raw_frames/hocky/fi67_xvid', 'data/raw_frames/hocky/fi68_xvid', 'data/raw_frames/hocky/fi69_xvid', 'data/raw_frames/hocky/fi6_xvid', 'data/raw_frames/hocky/fi71_xvid', 'data/raw_frames/hocky/fi70_xvid', 'data/raw_frames/hocky/fi72_xvid', 'data/raw_frames/hocky/fi73_xvid', 'data/raw_frames/hocky/fi77_xvid', 'data/raw_frames/hocky/fi74_xvid', 'data/raw_frames/hocky/fi76_xvid', 'data/raw_frames/hocky/fi75_xvid', 'data/raw_frames/hocky/fi79_xvid', 'data/raw_frames/hocky/fi78_xvid', 'data/raw_frames/hocky/fi7_xvid', 'data/raw_frames/hocky/fi80_xvid', 'data/raw_frames/hocky/fi81_xvid', 'data/raw_frames/hocky/fi83_xvid', 'data/raw_frames/hocky/fi82_xvid', 'data/raw_frames/hocky/fi84_xvid', 'data/raw_frames/hocky/fi85_xvid', 'data/raw_frames/hocky/fi86_xvid', 'data/raw_frames/hocky/fi87_xvid', 'data/raw_frames/hocky/fi88_xvid', 'data/raw_frames/hocky/fi89_xvid', 'data/raw_frames/hocky/fi8_xvid', 'data/raw_frames/hocky/fi90_xvid', 'data/raw_frames/hocky/fi92_xvid', 'data/raw_frames/hocky/fi93_xvid', 'data/raw_frames/hocky/fi91_xvid', 'data/raw_frames/hocky/fi94_xvid', 'data/raw_frames/hocky/fi95_xvid', 'data/raw_frames/hocky/fi96_xvid', 'data/raw_frames/hocky/fi98_xvid', 'data/raw_frames/hocky/fi97_xvid', 'data/raw_frames/hocky/fi99_xvid', 'data/raw_frames/hocky/fi9_xvid', 'data/raw_frames/hocky/no100_xvid', 'data/raw_frames/hocky/no101_xvid', 'data/raw_frames/hocky/no102_xvid', 'data/raw_frames/hocky/no104_xvid', 'data/raw_frames/hocky/no103_xvid', 'data/raw_frames/hocky/no105_xvid', 'data/raw_frames/hocky/no107_xvid', 'data/raw_frames/hocky/no108_xvid', 'data/raw_frames/hocky/no106_xvid', 'data/raw_frames/hocky/no10_xvid', 'data/raw_frames/hocky/no109_xvid', 'data/raw_frames/hocky/no110_xvid', 'data/raw_frames/hocky/no111_xvid', 'data/raw_frames/hocky/no114_xvid', 'data/raw_frames/hocky/no113_xvid', 'data/raw_frames/hocky/no112_xvid', 'data/raw_frames/hocky/no115_xvid', 'data/raw_frames/hocky/no116_xvid', 'data/raw_frames/hocky/no117_xvid', 'data/raw_frames/hocky/no118_xvid', 'data/raw_frames/hocky/no119_xvid', 'data/raw_frames/hocky/no11_xvid', 'data/raw_frames/hocky/no123_xvid', 'data/raw_frames/hocky/no121_xvid', 'data/raw_frames/hocky/no120_xvid', 'data/raw_frames/hocky/no122_xvid', 'data/raw_frames/hocky/no124_xvid', 'data/raw_frames/hocky/no125_xvid', 'data/raw_frames/hocky/no126_xvid', 'data/raw_frames/hocky/no128_xvid', 'data/raw_frames/hocky/no127_xvid', 'data/raw_frames/hocky/no130_xvid', 'data/raw_frames/hocky/no132_xvid', 'data/raw_frames/hocky/no131_xvid', 'data/raw_frames/hocky/no133_xvid', 'data/raw_frames/hocky/no12_xvid', 'data/raw_frames/hocky/no134_xvid', 'data/raw_frames/hocky/no135_xvid', 'data/raw_frames/hocky/no129_xvid', 'data/raw_frames/hocky/no136_xvid', 'data/raw_frames/hocky/no138_xvid', 'data/raw_frames/hocky/no13_xvid', 'data/raw_frames/hocky/no139_xvid', 'data/raw_frames/hocky/no140_xvid', 'data/raw_frames/hocky/no141_xvid', 'data/raw_frames/hocky/no142__xvid', 'data/raw_frames/hocky/no145_xvid', 'data/raw_frames/hocky/no143_xvid', 'data/raw_frames/hocky/no137_xvid', 'data/raw_frames/hocky/no144_xvid', 'data/raw_frames/hocky/no147_xvid', 'data/raw_frames/hocky/no148_xvid', 'data/raw_frames/hocky/no146_xvid', 'data/raw_frames/hocky/no149_xvid', 'data/raw_frames/hocky/no14_xvid', 'data/raw_frames/hocky/no150_xvid', 'data/raw_frames/hocky/no151_xvid', 'data/raw_frames/hocky/no153_xvid', 'data/raw_frames/hocky/no152_xvid', 'data/raw_frames/hocky/no154_xvid', 'data/raw_frames/hocky/no155_xvid', 'data/raw_frames/hocky/no156_xvid', 'data/raw_frames/hocky/no157_xvid', 'data/raw_frames/hocky/no159_xvid', 'data/raw_frames/hocky/no15_xvid', 'data/raw_frames/hocky/no158_xvid', 'data/raw_frames/hocky/no160_xvid', 'data/raw_frames/hocky/no162_xvid', 'data/raw_frames/hocky/no161_xvid', 'data/raw_frames/hocky/no163_xvid', 'data/raw_frames/hocky/no165_xvid', 'data/raw_frames/hocky/no164_xvid', 'data/raw_frames/hocky/no166_xvid', 'data/raw_frames/hocky/no167_xvid', 'data/raw_frames/hocky/no168_xvid', 'data/raw_frames/hocky/no169_xvid', 'data/raw_frames/hocky/no170_xvid', 'data/raw_frames/hocky/no16_xvid', 'data/raw_frames/hocky/no171_xvid', 'data/raw_frames/hocky/no172_xvid', 'data/raw_frames/hocky/no173_xvid', 'data/raw_frames/hocky/no174_xvid', 'data/raw_frames/hocky/no175_xvid', 'data/raw_frames/hocky/no176_xvid', 'data/raw_frames/hocky/no177_xvid', 'data/raw_frames/hocky/no178_xvid', 'data/raw_frames/hocky/no179_xvid', 'data/raw_frames/hocky/no17_xvid', 'data/raw_frames/hocky/no180_xvid', 'data/raw_frames/hocky/no181_xvid', 'data/raw_frames/hocky/no183_xvid', 'data/raw_frames/hocky/no182_xvid', 'data/raw_frames/hocky/no185_xvid', 'data/raw_frames/hocky/no184_xvid', 'data/raw_frames/hocky/no187_xvid', 'data/raw_frames/hocky/no186_xvid', 'data/raw_frames/hocky/no188_xvid', 'data/raw_frames/hocky/no189_xvid', 'data/raw_frames/hocky/no18_xvid', 'data/raw_frames/hocky/no191_xvid', 'data/raw_frames/hocky/no192_xvid', 'data/raw_frames/hocky/no190_xvid', 'data/raw_frames/hocky/no193_xvid', 'data/raw_frames/hocky/no194_xvid', 'data/raw_frames/hocky/no195_xvid', 'data/raw_frames/hocky/no196_xvid', 'data/raw_frames/hocky/no197_xvid', 'data/raw_frames/hocky/no198_xvid', 'data/raw_frames/hocky/no199_xvid', 'data/raw_frames/hocky/no1_xvid', 'data/raw_frames/hocky/no19_xvid', 'data/raw_frames/hocky/no200_xvid', 'data/raw_frames/hocky/no202_xvid', 'data/raw_frames/hocky/no201_xvid', 'data/raw_frames/hocky/no203_xvid', 'data/raw_frames/hocky/no205_xvid', 'data/raw_frames/hocky/no204_xvid', 'data/raw_frames/hocky/no206_xvid', 'data/raw_frames/hocky/no207_xvid', 'data/raw_frames/hocky/no208_xvid', 'data/raw_frames/hocky/no209_xvid', 'data/raw_frames/hocky/no20_xvid', 'data/raw_frames/hocky/no210_xvid', 'data/raw_frames/hocky/no212_xvid', 'data/raw_frames/hocky/no211_xvid', 'data/raw_frames/hocky/no213_xvid', 'data/raw_frames/hocky/no214_xvid', 'data/raw_frames/hocky/no215_xvid', 'data/raw_frames/hocky/no217_xvid', 'data/raw_frames/hocky/no216_xvid', 'data/raw_frames/hocky/no218_xvid', 'data/raw_frames/hocky/no219_xvid', 'data/raw_frames/hocky/no21_xvid', 'data/raw_frames/hocky/no220_xvid', 'data/raw_frames/hocky/no221_xvid', 'data/raw_frames/hocky/no222_xvid', 'data/raw_frames/hocky/no223_xvid', 'data/raw_frames/hocky/no224_xvid', 'data/raw_frames/hocky/no225_xvid', 'data/raw_frames/hocky/no226_xvid', 'data/raw_frames/hocky/no227_xvid', 'data/raw_frames/hocky/no228_xvid', 'data/raw_frames/hocky/no229_xvid', 'data/raw_frames/hocky/no22_xvid', 'data/raw_frames/hocky/no230_xvid', 'data/raw_frames/hocky/no231_xvid', 'data/raw_frames/hocky/no233_xvid', 'data/raw_frames/hocky/no232_xvid', 'data/raw_frames/hocky/no234_xvid', 'data/raw_frames/hocky/no235_xvid', 'data/raw_frames/hocky/no236_xvid', 'data/raw_frames/hocky/no237_xvid', 'data/raw_frames/hocky/no238mpg_xvid', 'data/raw_frames/hocky/no239_xvid', 'data/raw_frames/hocky/no240_xvid', 'data/raw_frames/hocky/no23_xvid', 'data/raw_frames/hocky/no242_xvid', 'data/raw_frames/hocky/no241_xvid', 'data/raw_frames/hocky/no243_xvid', 'data/raw_frames/hocky/no244_xvid', 'data/raw_frames/hocky/no245_xvid', 'data/raw_frames/hocky/no246_xvid', 'data/raw_frames/hocky/no248_xvid', 'data/raw_frames/hocky/no247_xvid', 'data/raw_frames/hocky/no249_xvid', 'data/raw_frames/hocky/no24_xvid', 'data/raw_frames/hocky/no250_xvid', 'data/raw_frames/hocky/no251_xvid', 'data/raw_frames/hocky/no252_xvid', 'data/raw_frames/hocky/no253_xvid', 'data/raw_frames/hocky/no254_xvid', 'data/raw_frames/hocky/no255_xvid', 'data/raw_frames/hocky/no256_xvid', 'data/raw_frames/hocky/no257_xvid', 'data/raw_frames/hocky/no258_xvid', 'data/raw_frames/hocky/no259_xvid', 'data/raw_frames/hocky/no25_xvid', 'data/raw_frames/hocky/no260_xvid', 'data/raw_frames/hocky/no261_xvid', 'data/raw_frames/hocky/no262_xvid', 'data/raw_frames/hocky/no263_xvid', 'data/raw_frames/hocky/no264_xvid', 'data/raw_frames/hocky/no265_xvid', 'data/raw_frames/hocky/no266_xvid', 'data/raw_frames/hocky/no267_xvid', 'data/raw_frames/hocky/no268_xvid', 'data/raw_frames/hocky/no269_xvid', 'data/raw_frames/hocky/no26_xvid', 'data/raw_frames/hocky/no270_xvid', 'data/raw_frames/hocky/no271_xvid', 'data/raw_frames/hocky/no273_xvid', 'data/raw_frames/hocky/no274_xvid', 'data/raw_frames/hocky/no272_xvid', 'data/raw_frames/hocky/no277_xvid', 'data/raw_frames/hocky/no275_xvid', 'data/raw_frames/hocky/no276_xvid', 'data/raw_frames/hocky/no278_xvid', 'data/raw_frames/hocky/no27_xvid', 'data/raw_frames/hocky/no279_xvid', 'data/raw_frames/hocky/no280_xvid', 'data/raw_frames/hocky/no282_xvid', 'data/raw_frames/hocky/no281_xvid', 'data/raw_frames/hocky/no283_xvid', 'data/raw_frames/hocky/no284_xvid', 'data/raw_frames/hocky/no285_xvid', 'data/raw_frames/hocky/no286_xvid', 'data/raw_frames/hocky/no287_xvid', 'data/raw_frames/hocky/no288_xvid', 'data/raw_frames/hocky/no289_xvid', 'data/raw_frames/hocky/no28_xvid', 'data/raw_frames/hocky/no290_xvid', 'data/raw_frames/hocky/no291_xvid', 'data/raw_frames/hocky/no292_xvid', 'data/raw_frames/hocky/no293_xvid', 'data/raw_frames/hocky/no294_xvid', 'data/raw_frames/hocky/no295_xvid', 'data/raw_frames/hocky/no296_xvid', 'data/raw_frames/hocky/no297_xvid', 'data/raw_frames/hocky/no298_xvid', 'data/raw_frames/hocky/no299_xvid', 'data/raw_frames/hocky/no29_xvid', 'data/raw_frames/hocky/no2_xvid', 'data/raw_frames/hocky/no300_xvid', 'data/raw_frames/hocky/no301_xvid', 'data/raw_frames/hocky/no302_xvid', 'data/raw_frames/hocky/no303_xvid', 'data/raw_frames/hocky/no304_xvid', 'data/raw_frames/hocky/no305_xvid', 'data/raw_frames/hocky/no306_xvid', 'data/raw_frames/hocky/no307_xvid', 'data/raw_frames/hocky/no308_xvid', 'data/raw_frames/hocky/no309_xvid', 'data/raw_frames/hocky/no30_xvid', 'data/raw_frames/hocky/no310_xvid', 'data/raw_frames/hocky/no311_xvid', 'data/raw_frames/hocky/no312_xvid', 'data/raw_frames/hocky/no313_xvid', 'data/raw_frames/hocky/no314_xvid', 'data/raw_frames/hocky/no315_xvid', 'data/raw_frames/hocky/no316_xvid', 'data/raw_frames/hocky/no317_xvid', 'data/raw_frames/hocky/no318_xvid', 'data/raw_frames/hocky/no319_xvid', 'data/raw_frames/hocky/no31_xvid', 'data/raw_frames/hocky/no320_xvid', 'data/raw_frames/hocky/no321_xvid', 'data/raw_frames/hocky/no323_xvid', 'data/raw_frames/hocky/no322_xvid', 'data/raw_frames/hocky/no324_xvid', 'data/raw_frames/hocky/no325_xvid', 'data/raw_frames/hocky/no327_xvid', 'data/raw_frames/hocky/no326_xvid', 'data/raw_frames/hocky/no328_xvid', 'data/raw_frames/hocky/no329_xvid', 'data/raw_frames/hocky/no32_xvid', 'data/raw_frames/hocky/no331_xvid', 'data/raw_frames/hocky/no332_xvid', 'data/raw_frames/hocky/no330_xvid', 'data/raw_frames/hocky/no334_xvid', 'data/raw_frames/hocky/no333_xvid', 'data/raw_frames/hocky/no335_xvid', 'data/raw_frames/hocky/no336_xvid', 'data/raw_frames/hocky/no338_xvid', 'data/raw_frames/hocky/no337_xvid', 'data/raw_frames/hocky/no339_xvid', 'data/raw_frames/hocky/no340_xvid', 'data/raw_frames/hocky/no33_xvid', 'data/raw_frames/hocky/no341_xvid', 'data/raw_frames/hocky/no342_xvid', 'data/raw_frames/hocky/no343_xvid', 'data/raw_frames/hocky/no344_xvid', 'data/raw_frames/hocky/no345_xvid', 'data/raw_frames/hocky/no346_xvid', 'data/raw_frames/hocky/no347_xvid', 'data/raw_frames/hocky/no348_xvid', 'data/raw_frames/hocky/no349_xvid', 'data/raw_frames/hocky/no34_xvid', 'data/raw_frames/hocky/no350_xvid', 'data/raw_frames/hocky/no351_xvid', 'data/raw_frames/hocky/no352_xvid', 'data/raw_frames/hocky/no354_xvid', 'data/raw_frames/hocky/no353_xvid', 'data/raw_frames/hocky/no355_xvid', 'data/raw_frames/hocky/no356_xvid', 'data/raw_frames/hocky/no358_xvid', 'data/raw_frames/hocky/no357_xvid', 'data/raw_frames/hocky/no359_xvid', 'data/raw_frames/hocky/no35_xvid', 'data/raw_frames/hocky/no360_xvid', 'data/raw_frames/hocky/no361_xvid', 'data/raw_frames/hocky/no362_xvid', 'data/raw_frames/hocky/no363_xvid', 'data/raw_frames/hocky/no364_xvid', 'data/raw_frames/hocky/no365_xvid', 'data/raw_frames/hocky/no366_xvid', 'data/raw_frames/hocky/no367_xvid', 'data/raw_frames/hocky/no368_xvid', 'data/raw_frames/hocky/no369_xvid', 'data/raw_frames/hocky/no36_xvid', 'data/raw_frames/hocky/no370_xvid', 'data/raw_frames/hocky/no372_xvid', 'data/raw_frames/hocky/no371_xvid', 'data/raw_frames/hocky/no373_xvid', 'data/raw_frames/hocky/no374_xvid', 'data/raw_frames/hocky/no375_xvid', 'data/raw_frames/hocky/no376_xvid', 'data/raw_frames/hocky/no377_xvid', 'data/raw_frames/hocky/no378_xvid', 'data/raw_frames/hocky/no379_xvid', 'data/raw_frames/hocky/no37_xvid', 'data/raw_frames/hocky/no381_xvid', 'data/raw_frames/hocky/no380_xvid', 'data/raw_frames/hocky/no382_xvid', 'data/raw_frames/hocky/no384_xvid', 'data/raw_frames/hocky/no383_xvid', 'data/raw_frames/hocky/no385_xvid', 'data/raw_frames/hocky/no386_xvid', 'data/raw_frames/hocky/no388_xvid', 'data/raw_frames/hocky/no387_xvid', 'data/raw_frames/hocky/no389_xvid', 'data/raw_frames/hocky/no38_xvid', 'data/raw_frames/hocky/no390_xvid', 'data/raw_frames/hocky/no391_xvid', 'data/raw_frames/hocky/no392_xvid', 'data/raw_frames/hocky/no393_xvid', 'data/raw_frames/hocky/no394_xvid', 'data/raw_frames/hocky/no395_xvid', 'data/raw_frames/hocky/no396_xvid', 'data/raw_frames/hocky/no397_xvid', 'data/raw_frames/hocky/no398_xvid', 'data/raw_frames/hocky/no399_xvid', 'data/raw_frames/hocky/no3_xvid', 'data/raw_frames/hocky/no39_xvid', 'data/raw_frames/hocky/no400_xvid', 'data/raw_frames/hocky/no403_xvid', 'data/raw_frames/hocky/no401_xvid', 'data/raw_frames/hocky/no402_xvid', 'data/raw_frames/hocky/no404_xvid', 'data/raw_frames/hocky/no405_xvid', 'data/raw_frames/hocky/no406_xvid', 'data/raw_frames/hocky/no407_xvid', 'data/raw_frames/hocky/no408_xvid', 'data/raw_frames/hocky/no409_xvid', 'data/raw_frames/hocky/no40_xvid', 'data/raw_frames/hocky/no411_xvid', 'data/raw_frames/hocky/no413_xvid', 'data/raw_frames/hocky/no410_xvid', 'data/raw_frames/hocky/no412_xvid', 'data/raw_frames/hocky/no414_xvid', 'data/raw_frames/hocky/no415_xvid', 'data/raw_frames/hocky/no418_xvid', 'data/raw_frames/hocky/no416_xvid', 'data/raw_frames/hocky/no419_xvid', 'data/raw_frames/hocky/no417_xvid', 'data/raw_frames/hocky/no420_xvid', 'data/raw_frames/hocky/no41_xvid', 'data/raw_frames/hocky/no422_xvid', 'data/raw_frames/hocky/no424_xvid', 'data/raw_frames/hocky/no423_xvid', 'data/raw_frames/hocky/no426_xvid', 'data/raw_frames/hocky/no425_xvid', 'data/raw_frames/hocky/no428_xvid', 'data/raw_frames/hocky/no421_xvid', 'data/raw_frames/hocky/no429_xvid', 'data/raw_frames/hocky/no42_xvid', 'data/raw_frames/hocky/no430_xvid', 'data/raw_frames/hocky/no427_xvid', 'data/raw_frames/hocky/no431_xvid', 'data/raw_frames/hocky/no432_xvid', 'data/raw_frames/hocky/no433_xvid', 'data/raw_frames/hocky/no434_xvid', 'data/raw_frames/hocky/no435_xvid', 'data/raw_frames/hocky/no436_xvid', 'data/raw_frames/hocky/no437_xvid', 'data/raw_frames/hocky/no438_xvid', 'data/raw_frames/hocky/no439_xvid', 'data/raw_frames/hocky/no43_xvid', 'data/raw_frames/hocky/no440_xvid', 'data/raw_frames/hocky/no441_xvid', 'data/raw_frames/hocky/no442_xvid', 'data/raw_frames/hocky/no443_xvid', 'data/raw_frames/hocky/no444_xvid', 'data/raw_frames/hocky/no445_xvid', 'data/raw_frames/hocky/no446_xvid', 'data/raw_frames/hocky/no447_xvid', 'data/raw_frames/hocky/no448_xvid', 'data/raw_frames/hocky/no44_xvid', 'data/raw_frames/hocky/no449_xvid', 'data/raw_frames/hocky/no450_xvid', 'data/raw_frames/hocky/no451_xvid', 'data/raw_frames/hocky/no452_xvid', 'data/raw_frames/hocky/no453_xvid', 'data/raw_frames/hocky/no454_xvid', 'data/raw_frames/hocky/no456_xvid', 'data/raw_frames/hocky/no457_xvid', 'data/raw_frames/hocky/no458_xvid', 'data/raw_frames/hocky/no459_xvid', 'data/raw_frames/hocky/no455_xvid', 'data/raw_frames/hocky/no45_xvid', 'data/raw_frames/hocky/no460_xvid', 'data/raw_frames/hocky/no461_xvid', 'data/raw_frames/hocky/no462_xvid', 'data/raw_frames/hocky/no463_xvid', 'data/raw_frames/hocky/no464_xvid', 'data/raw_frames/hocky/no465_xvid', 'data/raw_frames/hocky/no466_xvid', 'data/raw_frames/hocky/no467_xvid', 'data/raw_frames/hocky/no468_xvid', 'data/raw_frames/hocky/no469_xvid', 'data/raw_frames/hocky/no46_xvid', 'data/raw_frames/hocky/no470_xvid', 'data/raw_frames/hocky/no471_xvid', 'data/raw_frames/hocky/no472_xvid', 'data/raw_frames/hocky/no473_xvid', 'data/raw_frames/hocky/no474_xvid', 'data/raw_frames/hocky/no475_xvid', 'data/raw_frames/hocky/no476_xvid', 'data/raw_frames/hocky/no477_xvid', 'data/raw_frames/hocky/no478_xvid', 'data/raw_frames/hocky/no479_xvid', 'data/raw_frames/hocky/no47_xvid', 'data/raw_frames/hocky/no480_xvid', 'data/raw_frames/hocky/no481_xvid', 'data/raw_frames/hocky/no482_xvid', 'data/raw_frames/hocky/no483_xvid', 'data/raw_frames/hocky/no484_xvid', 'data/raw_frames/hocky/no485_xvid', 'data/raw_frames/hocky/no486_xvid', 'data/raw_frames/hocky/no487_xvid', 'data/raw_frames/hocky/no488_xvid', 'data/raw_frames/hocky/no48_xvid', 'data/raw_frames/hocky/no489_xvid', 'data/raw_frames/hocky/no490_xvid', 'data/raw_frames/hocky/no491_xvid', 'data/raw_frames/hocky/no493_xvid', 'data/raw_frames/hocky/no492_xvid', 'data/raw_frames/hocky/no494_xvid', 'data/raw_frames/hocky/no495_xvid', 'data/raw_frames/hocky/no496_xvid', 'data/raw_frames/hocky/no497_xvid', 'data/raw_frames/hocky/no499_xvid', 'data/raw_frames/hocky/no498_xvid', 'data/raw_frames/hocky/no4_xvid', 'data/raw_frames/hocky/no49_xvid', 'data/raw_frames/hocky/no500_xvid', 'data/raw_frames/hocky/no52_xvid', 'data/raw_frames/hocky/no51_xvid', 'data/raw_frames/hocky/no50_xvid', 'data/raw_frames/hocky/no53_xvid', 'data/raw_frames/hocky/no54_xvid', 'data/raw_frames/hocky/no55_xvid', 'data/raw_frames/hocky/no57_xvid', 'data/raw_frames/hocky/no56_xvid', 'data/raw_frames/hocky/no5_xvid', 'data/raw_frames/hocky/no60_xvid', 'data/raw_frames/hocky/no58_xvid', 'data/raw_frames/hocky/no59_xvid', 'data/raw_frames/hocky/no61_xvid', 'data/raw_frames/hocky/no63_xvid', 'data/raw_frames/hocky/no62_xvid', 'data/raw_frames/hocky/no64_xvid', 'data/raw_frames/hocky/no65___xvid', 'data/raw_frames/hocky/no66_xvid', 'data/raw_frames/hocky/no68_xvid', 'data/raw_frames/hocky/no67_xvid', 'data/raw_frames/hocky/no69_xvid', 'data/raw_frames/hocky/no6_xvid', 'data/raw_frames/hocky/no70_xvid', 'data/raw_frames/hocky/no71_xvid', 'data/raw_frames/hocky/no72_xvid', 'data/raw_frames/hocky/no74_xvid', 'data/raw_frames/hocky/no76_xvid', 'data/raw_frames/hocky/no73_xvid', 'data/raw_frames/hocky/no75_xvid', 'data/raw_frames/hocky/no77_xvid', 'data/raw_frames/hocky/no78_xvid', 'data/raw_frames/hocky/no79_xvid', 'data/raw_frames/hocky/no80_xvid', 'data/raw_frames/hocky/no7_xvid', 'data/raw_frames/hocky/no81_xvid', 'data/raw_frames/hocky/no82_xvid', 'data/raw_frames/hocky/no83_xvid', 'data/raw_frames/hocky/no84_xvid', 'data/raw_frames/hocky/no85_xvid', 'data/raw_frames/hocky/no86_xvid', 'data/raw_frames/hocky/no87_xvid', 'data/raw_frames/hocky/no88_xvid', 'data/raw_frames/hocky/no89_xvid', 'data/raw_frames/hocky/no8_xvid', 'data/raw_frames/hocky/no90_xvid', 'data/raw_frames/hocky/no92_xvid', 'data/raw_frames/hocky/no91_xvid', 'data/raw_frames/hocky/no93_xvid', 'data/raw_frames/hocky/no94_xvid', 'data/raw_frames/hocky/no95_xvid', 'data/raw_frames/hocky/no96_xvid', 'data/raw_frames/hocky/no98_xvid', 'data/raw_frames/hocky/no97_xvid', 'data/raw_frames/hocky/no9_xvid', 'data/raw_frames/hocky/no99_xvid']\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdOnJUZVQ6Ga"
      },
      "source": [
        "# **Train / Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEo2N_RVQLwi",
        "outputId": "1284cc20-01cb-4ae9-be0f-c49ac5f7da10"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#split up our data into a  training and test set\n",
        "train_path, test_path, train_y, test_y =  train_test_split(videos_frames_paths,videos_labels, test_size=0.20, random_state=42)\n",
        "\n",
        "# if apply_aug:\n",
        "#     aug_paths = []\n",
        "#     aug_y = []\n",
        "#     for train_path_, train_y_ in zip(train_path,train_y):\n",
        "#         aug_path = generate_augmentations(train_path_,force = False)\n",
        "#         aug_paths.append(aug_path)\n",
        "#         aug_y.append(train_y_)\n",
        "#\n",
        "#     train_path = train_path + aug_paths\n",
        "#     train_y = train_y + aug_y\n",
        "\n",
        "train_path, valid_path, train_y, valid_y = train_test_split(train_path, train_y, test_size=0.20, random_state=42)\n",
        "print(\"Dataset splitted\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset splitted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6kpF54nhgLj"
      },
      "source": [
        "----------------------------------------------------------------------------\n",
        "# **GET SEQUENCES:** \n",
        "it makes sequence of adjacent images\n",
        "\n",
        "Requires:\n",
        "- FRAME_LOADER\n",
        "- CROP_IMG_REMOVE_DARK: removes black border background from image\n",
        "- CROP_IMG: cuts the image to a random side (Center, left up, left down, right up, right down)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6Aa7XdEhcNY"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.preprocessing import image\n",
        "import random\n",
        "corner_keys = [\"Center\",\"Left_up\",\"Left_down\",\"Right_up\",\"Right_down\"]\n",
        "\n",
        "def frame_loader(frames,figure_shape,to_norm = True):\n",
        "    output_frames = []\n",
        "    for frame in frames:\n",
        "        image = load_img(frame, target_size=(figure_shape, figure_shape),interpolation='bilinear')\n",
        "        img_arr = img_to_array(image)\n",
        "        # Scale\n",
        "        figure = (img_arr / 255.).astype(np.float32)\n",
        "        # Normalize\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "        figure = (figure - mean) / std\n",
        "        output_frames.append(figure)\n",
        "    return output_frames\n",
        "\n",
        "\n",
        "def crop_img__remove_Dark(img, x_crop,y_crop, x,y, figure_size):\n",
        "    x_start = x_crop\n",
        "    x_end = x-x_crop\n",
        "    y_start = y_crop\n",
        "    y_end = y-y_crop\n",
        "    return cv2.resize(img[y_start:y_end,x_start:x_end,:],(figure_size,figure_size))\n",
        "\n",
        "\n",
        "def crop_img(img, figure_shape, percentage=0.8, corner=\"Left_up\"):\n",
        "    if (corner == None):\n",
        "        corner = random.choice(corner_keys)\n",
        "    if corner not in corner_keys:\n",
        "        raise ValueError('Invalid corner method {} specified. Supported corners are {}'.format(corner, \", \".join(corner_keys)))\n",
        "\n",
        "    resize = int(figure_shape*percentage)\n",
        "\n",
        "    if (corner ==\"Left_up\"):\n",
        "        x_start = 0\n",
        "        x_end = resize\n",
        "        y_start = 0\n",
        "        y_end = resize\n",
        "    if (corner == \"Right_down\"):\n",
        "        x_start = figure_shape-resize\n",
        "        x_end = figure_shape\n",
        "        y_start = figure_shape-resize\n",
        "        y_end = figure_shape\n",
        "    if (corner ==\"Right_up\"):\n",
        "        x_start = 0\n",
        "        x_end = resize\n",
        "        y_start = figure_shape-resize\n",
        "        y_end = figure_shape\n",
        "    if (corner == \"Left_down\"):\n",
        "        x_start = figure_shape-resize\n",
        "        x_end = figure_shape\n",
        "        y_start = 0\n",
        "        y_end = resize\n",
        "    if (corner == \"Center\"):\n",
        "        half = int(figure_shape*(1-percentage))\n",
        "        x_start = half\n",
        "        x_end = figure_shape-half\n",
        "        y_start = half\n",
        "        y_end = figure_shape-half\n",
        "\n",
        "    img = cv2.resize(img[y_start:y_end,x_start:x_end, :], (figure_shape, figure_shape)).astype(np.float32)\n",
        "    return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8Vpeh0Fn-Wk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995f21f3-d652-4fa9-962b-a0dd18616368"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import scipy\n",
        "import glob\n",
        "Debug_Print_AUG=True     # True to save augmented images\n",
        "\n",
        "def get_sequences(data_paths, labels, figure_shape, seq_length, classes=1, use_augmentation = False, use_crop=True, crop_x_y=None):\n",
        "    X, y = [], []\n",
        "    seq_len = 0\n",
        "    for data_path, label in zip(data_paths,labels):\n",
        "        frames = sorted(glob.glob(os.path.join(data_path, '*jpg')))\n",
        "        x = frame_loader(frames, figure_shape)\n",
        "\n",
        "        if (crop_x_y):                                                            #remove dark from background\n",
        "            x = [crop_img__remove_Dark(x_,crop_x_y[0],crop_x_y[1],x_.shape[0],x_.shape[1],figure_shape) for x_ in x]\n",
        "        \n",
        "        if use_augmentation:                                                      #data augmentation\n",
        "            rand = scipy.random.random()\n",
        "            corner=\"\"\n",
        "            if rand > 0.5:\n",
        "                if (use_crop):\n",
        "                    corner= random.choice(corner_keys)\n",
        "                    x = [crop_img(x_,figure_shape, 0.7, corner) for x_ in x]        #crop random corner of an image\n",
        "                x = [frame.transpose(1, 0, 2) for frame in x]                     #transpose \n",
        "\n",
        "                if (Debug_Print_AUG):                                             #to save augm. image on disk\n",
        "                    to_write = [list(a) for a in zip(frames, x)]\n",
        "                    [cv2.imwrite(x_[0] + \"_\" + corner, x_[1] * 255) for x_ in to_write]\n",
        "\n",
        "        x = [x[i] - x[i+1] for i in range(len(x)-1)]                              #subtraction of adjacent frames\n",
        "        X.append(x)\n",
        "        y.append(label)\n",
        "    X = pad_sequences(X, maxlen=seq_length, padding='pre', truncating='pre')\n",
        "    \n",
        "    if classes > 1:                        #only for categorical crossentropy\n",
        "        x_ = to_categorical(x_,classes)\n",
        "    return np.array(X), np.array(y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-039f1e2ca170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mDebug_Print_AUG\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m     \u001b[0;31m# True to save augmented images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'to_categorical' from 'keras.utils' (/usr/local/lib/python3.7/dist-packages/keras/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fkod9PtewBy"
      },
      "source": [
        "# **DATA GENERATOR** \n",
        "- requires GET SEQUENCES\n",
        "\n",
        "Data generators allow to feed the model with large data, with the yield that returns part of the data gradually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eFe062khBr6"
      },
      "source": [
        "from numpy.random import shuffle\n",
        "def data_generator(data_paths, labels, batch_size, figure_shape, seq_length, use_aug, use_crop, crop_x_y, classes = 1):\n",
        "    while True:\n",
        "        indexes = np.arange(len(data_paths))\n",
        "        np.random.shuffle(indexes)\n",
        "        select_indexes = indexes[:batch_size]\n",
        "        data_paths_batch = [data_paths[i] for i in select_indexes]\n",
        "        labels_batch = [labels[i] for i in select_indexes]\n",
        "\n",
        "        X, y = get_sequences(data_paths_batch, labels_batch, figure_shape, seq_length, classes, use_augmentation = use_aug, use_crop=use_crop, crop_x_y=crop_x_y)\n",
        "\n",
        "        yield X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4hkgqu3Qfpf"
      },
      "source": [
        "crop_dark = dict(\n",
        "    hocky = (11, 38),\n",
        "    violentflow = None,\n",
        "    movies = None\n",
        ")\n",
        "\n",
        "crop_x_y = None\n",
        "if (crop_dark):   #None\n",
        "    crop_x_y = crop_dark[dataset_name]\n",
        "\n",
        "batch_size = 2                  #16\n",
        "len_train, len_valid = len(train_path), len(valid_path)       #fig size #fix len    #False\n",
        "print(len_train, len_valid, len(test_path))\n",
        "\n",
        "train_gen =    data_generator(train_path, train_y, batch_size, 244, 20, use_aug=True,  use_crop=True,  crop_x_y=crop_x_y, classes=1)\n",
        "validate_gen = data_generator(valid_path, valid_y, batch_size, 244, 20, use_aug=False, use_crop=False, crop_x_y=crop_x_y, classes=1)\n",
        "\n",
        "test_x, test_y = get_sequences(test_path, test_y, 244, 20, crop_x_y=crop_x_y, classes=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJT47wWL5yu7"
      },
      "source": [
        "# **ARCHITECTURE**\n",
        "We describe the **architecture** build upon four type of layers, \n",
        "- The **first** is the **input layer** that receive a sequence of 10 frames that are a computed difference of two adjacent frames from the original video.\n",
        "\n",
        "- The **second** type of **layers** belongs to a **Resnet50 CNN network** that aim to classify images, the initial weights of the layers are taken form a pre-trained model on **image-net**, the CNN process each frame separately and during training the weights of the network are shared. \n",
        "\n",
        "- The **third layer** is the **Convolution LSTM (ConvLSTM)** where each frame from the CNN enters into a **ConvLSTM** cell with an hidden state of **256 convolution filters** of **size 3**. \n",
        "\n",
        "- The **forth** type of **layers** process the ConvLSTM and **output** the **binary prediction**, a **Max pooling layer** of **size 2** reduces the data and chooses the most informational pixels, then the data is **batch normalized** and connected to a series of **fully connected layer** of sizes **1000, 256, 10** and finally a **binary output perception** with a **sigmoid activation function**. \n",
        "- **Between** each of the **fully connected layers** we use** RELU activation**.\n",
        "\n",
        "- We use **binary cross entropy** as our **loss function** and **RMSprop** as an **optimizer**, **20%** of the data is select for **validation** and rest **80%** is selected to **train**. \n",
        "\n",
        "- The **learning rate** of the network **starts** with value of **0.0001** and is **reduced** by **half** **after 5 epochs** of **no improvement** in the **validation loss**. \n",
        "\n",
        "- We train the model with **50 epochs** but also use **early stopping** in case where the **network validation loss haven't improve for 15 epochs.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnCSPzD5w4Jh"
      },
      "source": [
        "model fit: # the network is trained on data generatores and apply the callacks when the validation loss is not improving:\n",
        "- 1) early stop to training after n iteration\n",
        "- 2) reducing the learning rate after k iteration where k< n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np600e_tZsvB"
      },
      "source": [
        "import tensorflow\n",
        "from numpy.random import seed\n",
        "from keras import Input\n",
        "from keras.applications import ResNet50\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Dense, Flatten, Dropout, ConvLSTM2D, BatchNormalization, Activation, Reshape\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "from keras.layers.convolutional import (MaxPooling2D)\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from keras import backend as K    #added\n",
        "K.clear_session()                 #added\n",
        "\n",
        "tensorflow.random.set_seed(2)\n",
        "seed(1)\n",
        "\n",
        "initial_weights = 'glorot_uniform'       #'Xavier'\n",
        "\n",
        "input_layer = Input(shape=(20, 244, 244, 3))\n",
        "cnn = ResNet50(weights= 'imagenet' , include_top=False, input_shape =(244, 244, 3))                   #VGG19 (without input shape)\n",
        "for layer in cnn.layers:      #retrain\n",
        "    layer.trainable = True                  #False if static\n",
        "\n",
        "dropout = 0.0           #0.5\n",
        "cnn = TimeDistributed(cnn)(input_layer)\n",
        "\n",
        "# cnn = Reshape((20,4, 4, 128), input_shape=(20,1, 1, 2048))(cnn)                               #the resnet output shape is 1,1,20148 and need to be reshape for the ConvLSTM filters\n",
        "lstm = ConvLSTM2D(filters=256, kernel_size=(3, 3), padding='same', return_sequences=False)(cnn)     #long short time memory 2 dimension\n",
        "lstm = MaxPooling2D(pool_size=(2, 2))(lstm)\n",
        "flat = Flatten()(lstm)\n",
        "\n",
        "flat = BatchNormalization()(flat)\n",
        "#flat = Dropout(dropout)(flat)      #don't use dropout\n",
        "linear = Dense(1000)(flat)\n",
        "\n",
        "relu = Activation('relu')(linear)\n",
        "linear = Dense(256)(relu)\n",
        "#linear = Dropout(dropout)(linear)\n",
        "relu = Activation('relu')(linear)\n",
        "linear = Dense(10)(relu)\n",
        "#linear = Dropout(dropout)(linear)\n",
        "relu = Activation('relu')(linear)\n",
        "predictions = Dense(1 , activation='sigmoid')(relu)     # classes = 1           #'softmax' if classes > 1\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=predictions)\n",
        "\n",
        "model.compile(optimizer= RMSprop(learning_rate = 0.0001), loss= 'binary_crossentropy', metrics=['accuracy'])       #Adam, 1e-4, 1e-3     #'categorical_crossentropy' if classes >1\n",
        "model.summary()\n",
        "\n",
        "batch_epoch_ratio = 0.5         #1\n",
        "\n",
        "hist = model.fit(train_gen, validation_data = validate_gen,\n",
        "    steps_per_epoch= int(float(len_train) / float(batch_size * batch_epoch_ratio)),\n",
        "    epochs = 50,                   \n",
        "    validation_steps = int(float(len_valid) / float(batch_size)),\n",
        "    callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.001, patience=15, ),\n",
        "                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5 , min_lr=1e-8, verbose=1)]\n",
        "                #TestCallback((test_x, test_y))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OToTEt131Yvc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekrjyNAL5-3v"
      },
      "source": [
        "# **EVALUATION**\n",
        "- The hyper-tunning process as mentioned in section 4.3 allows us to find the best performing parameters of the network based on the \"Hockey\" dataset. \n",
        "the chosen architecture is already presented in the section 4.1. \n",
        "- In Figure 6 we present the hyper-tunning test accuracy for each of the hyper-parameters values.\n",
        "- The best performing **CNN** is the **Resnet50** with **90% accuracy**, the InceptionV3 CNN was not far from the Resnet50 with 89% accuracy but the VGG19 CNN had poor results of only 79% accuracy.\n",
        "- The **starting learning rate** value had a critical effect on the network results where the 0.001 learning rate resolved with only 46% accuracy which is lower then the random classiffcation. \n",
        "as already mentioned by the original paper, the **learning rate** of **0.0001** had far better results in all experiments.\n",
        "- The **augmentation** increases the accuracy by** 4.5%** and smaller length size of the **sequence** improve the accuracy by **2%.** \n",
        "the dropout of 50% did no improve the model performance and results with only 86% accuracy. \n",
        "- As expected the static CNN configuration where the CNN weights are not retrained had very poor results of 59% accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7EdHY6-5X-G"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "score = model.evaluate(test_x, test_y, batch_size=2)\n",
        "print(model.metrics)\n",
        "print('Loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "########################### ACCURACY ###################################################\n",
        "plt.plot(hist.history['accuracy'], label='train')\n",
        "plt.plot(hist.history['val_accuracy'], label='val')\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "##################### LOSS ###############################\n",
        "plt.plot(hist.history['loss'], label='train')\n",
        "plt.plot(hist.history['val_loss'], label='val')\n",
        "#plt.title('model loss')\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhMmPT6FUudS"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion(y_true, y_pred):\n",
        "  conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "  plt.imshow(conf_matrix)\n",
        "  plt.xticks([0, 1], ['NonViolence', 'Violence'], fontsize=16)\n",
        "  plt.yticks([0, 1], ['NonViolence', 'Violence'], fontsize=16)\n",
        "  plt.ylabel('Truth', fontsize=20)\n",
        "  plt.xlabel('Prediction', fontsize=20)\n",
        "  plt.colorbar()\n",
        "\n",
        "  plt.grid(False)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ircs3NMKtbVH"
      },
      "source": [
        "prediction_probabilities = (model.predict(test_x, batch_size=2).ravel()>0.5)+0\n",
        "print(prediction_probabilities)\n",
        "print(test_y)\n",
        "plot_confusion(test_y, prediction_probabilities)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}